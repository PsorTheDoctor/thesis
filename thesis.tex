\documentclass{article}
\usepackage{newtxtext}
\usepackage[fontsize=9pt]{fontsize}
\usepackage{setspace}
\usepackage{geometry}
\usepackage{polski}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{listings}
\usepackage{physics}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{xpatch}
\usepackage{fancyhdr}
\usepackage{comment}
\graphicspath{{./images/}}

\newcommand\filling[1][4cm]{\makebox[#1]{\dotfill}}
\setstretch{1.5}
\geometry{a4paper, left=30mm, right=20mm, top=25mm, bottom=25mm}
\captionsetup[figure]{font=Large}
\setlength\parindent{1cm}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[C]{Rozpoznawanie i estymacja pozycji obiektu z użyciem systemu wizyjnego \\w zadaniu chwytania realizowanego przez manipulator}
\fancyfoot[C]{\large\thepage}

\makeatletter
\xpatchcmd{\tableofcontents}{\contentsname \@mkboth}{\LARGE\contentsname \@mkboth}{}{}
\makeatother

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{1,1,1}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\footnotesize\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\large,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}
\lstset{
  literate={ą}{{\k a}}1
  		   {Ą}{{\k A}}1
           {ż}{{\. z}}1
           {Ż}{{\. Z}}1
           {ź}{{\' z}}1
           {Ź}{{\' Z}}1
           {ć}{{\' c}}1
           {Ć}{{\' C}}1
           {ę}{{\k e}}1
           {Ę}{{\k E}}1
           {ó}{{\' o}}1
           {Ó}{{\' O}}1
           {ń}{{\' n}}1
           {Ń}{{\' N}}1
           {ś}{{\' s}}1
           {Ś}{{\' S}}1
           {ł}{{\l}}1
           {Ł}{{\L}}1
}

\begin{document}
\begin{center}
\huge{\textbf{POLITECHNIKA BIAŁOSTOCKA}} \\
\LARGE{\textbf{WYDZIAŁ MECHANICZNY}} 
\vfill
\huge{PRACA DYPLOMOWA INŻYNIERSKA}
\vfill
\end{center}

\LARGE{TEMAT: Rozpoznawanie i estymacja pozycji obiektu z użyciem systemu wizyjnego w zadaniu chwytania realizowanego przez manipulator}
\vfill
\begin{flushright}
\LARGE{WYKONAWCA: Adam Wołkowycki}
\vfill
\LARGE{PODPIS: \filling[4cm]}
\vfill
\end{flushright}
\LARGE{PROMOTOR: Dr Adam Wolniakowski}
\vfill
\LARGE{PODPIS: \filling[4cm]}
\vfill
\Large{}

\begin{center}
\Large{\textbf{BIAŁYSTOK 2022}}
\end{center}
\thispagestyle{empty}

\newpage
\section*{\LARGE{Abstract}}
In the recent years, a rising trend in application of autonomous robots can be observed. Nowadays, many solutions intended to this part of market are developed and each of these is based on solutions that are hard to algorithmizable. The examples of this kind of tasks are image processing, object position estimation and configuration of kinematics. I am going to focus on them in my thesis.

The purpose of this thesis is to develop a solution resposible for recognition a given object and perform grasping it by a manipulator. The further development of the problem known as \emph{bin picking} aims at increasing a scope of industrial robots' operation in a context of their work. It is quintessential in many modern robotic applications targeted for being more "flexible" and intuitive.

Because the tasks I mentioned are often performed by a particular type of robots, in my work I am going to use UR5 manipulator by Danish manufacturer \emph{Universal Robots}. There will be implemented algorithm on it that will be able to define a location of seen objects based on the data from the computer vision sensors. In my work I will be using code examples written in Python. Thanks to this, it will be possible to deeply verified in practice the working of a robot.

\newpage
\section*{\LARGE{Streszczenie}}
W ostatnich latach można zaobserwować wzrastający trend na zastosowanie robotów o działaniu autonomicznym. Powstaje obecnie wiele rozwiązań przeznaczonych dla tego segmentu rynku, a każdy z nich opiera swoje działanie o rozwiązania, które są przyjęte jako trudno algorytmizowalne. Przykładami takich zadań są przetwarzanie obrazu, estymacja położenia obiektów i konfiguracja kinematyki robota. Właśnie na nich zamieram skupić się w mojej pracy.

Celem pracy jest opracowanie rozwiązania odpowiedzialnego za rozpoznawanie wskazanego obiektu i realizację chwycenia go przy wykorzystaniu manipulatora. Rozwinięcie problemu znanego pod nazwą \emph{bin picking} ma przede wszystkim na celu zwiększenie zakresu działania robotów przemysłowych w kontekście ich pracy. Stanowi to kwintesencję w wielu współczesnych zastosowaniach robotyki dążącej do bycia bardziej "elastyczną" i intuicyjną.

Ponieważ zadania, o których wspomniałem są często realizowane przez określony rodzaj robotów, w swojej pracy zamierzam wykorzytać manipulator UR5 duńskiego producenta Universal Robots. Na nim zostanie zaimplementowany algorytm, który w oparciu o dane z systemu wizyjnego będzie w stanie określić położenie widzianych przedmiotów. W pracy posłużę się programami stworzonymi w języku Python. Dzięki temu działanie robota będzie można gruntownie zweryfikować w praktyce.

\newpage
\tableofcontents

\newpage
\section*{\LARGE{Wykaz oznaczeń}}
Podczas pisania pracy została użyta podana notacja.
\begin{itemize}
\item $\mathbf{A}$ - macierze oznaczone zostały pogrubionymi wielkimi literami
\item $\vb{v}$ - wektory oznaczone zostały pogrubionymi małymi literami
\item $s$ - wartości skalarne oznaczone zostały małymi literami
\item $\mathbb{R}$ - zbiory liczb oznaczone zostały konturowymi wielkimi literami
\item $\mathbf{R}$ - macierz rotacji (obrotu) 3 $\times$ 3
\item $\vb{t}$ - wektor translacji (przesunięcia) 1 $\times$ 3
\item $\mathbf{T}$ - macierz transformacji 4 $\times$ 4
\end{itemize}

\newpage
\section*{\LARGE{1. Wprowadzenie}} \addcontentsline{toc}{section}{1. Wprowadzenie}

Definiując robotykę jako dziedzinę wiedzy mamy na uwadze zajmowanie się urządzeniami mechanicznymi, które mogą być przystosowane do realizacji wielu różnych zadań przez zmianę programu nimi sterującego. Jest to podstawowa różnica pozwalająca wyodrębnić robotykę na tle sztywnej automatyzacji zajmującej się maszynami przeznaczonymi do wykonywania jednego rodzaju zadań przez dłuższy czas. Współczesna robotyka stanowi przede wszystkim zbiór pewnych rozwiązań ukierunkowanych na tworzenie i sterowanie robotami. Mając to na uwadze, możemy wyróżnić dwa problemy: natury konstrukcyjnej i sterowania. Ponieważ wielu producentów z branży robotyki oferuje szeroką gamę rozwiązań spełniających potrzeby niniejszej pracy, posłużymy się manipulatorem przegubowym - najbardziej powszechną odmianą robota przemysłowego, w którego konstrukcję nie będziemy ingerować. Dlatego, aby zrozumieć koncepcje zawarte w tej pracy nie jest wymagana dogłębna znajomość budowy robota. Natomiast, jeśli chodzi o sterowanie to możemy ponownie wyodrębnić dwie podrzędne kategorie: manipulację i lokomocję. 

\begin{itemize}
\item \textbf{Manipulację} definiujemy jako zdolność do wykonywania precyzyjnych ruchów przez efektor końcowy robota w celu osiągnięcia wyznaczonego celu, np. przeniesienia przedmiotu procesowanego - tzw. zadanie \emph{pick and place}. Roboty posiadające tą cechę nazywamy mianem manipulatorów.

\item \textbf{Lokomocja} z kolei, jak sama nazwa wskazuje, opisuje zdolność do przemieszczania się platformy mobilnej. Jest kluczowa m.in. dla pojazdów AVG \emph{(Automated Guided Vehicles)}, łazików czy robotów kroczących. Ponieważ w swojej pracy zamierzam skupić się na robotach stacjonarnych, nie będę wracał więcej do tego tematu.
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=10cm]{classification}
\caption{Podział robotyki na odrębne zagadnienia.}
\end{figure}

Musimy jednak mieć na uwadze, że jest to dość ogólna i luźna klasyfikacja. Pierwotnie, definicję wyodrębniającą robotykę i automatyzację zaproponował John J. Craig, natomiast podział na manipulację i lokomocję został został przytoczony w książce Marca H. Raiberta \emph{Legged Robots That Balance} w ostatnim rozdziale w pytaniu \emph{Do Locomotion and Manipulation Have a Common Ground?} Tam też problem został rozwinięty w kontekście badań nad lokomocją i manipulacją oraz tego jak te dwie dziedziny wpływają na siebie nawzajem.

\subsection*{\LARGE{Manipulacja a percepcja}} \addcontentsline{toc}{subsection}{Manipulacja a percepcja}

Gdy mamy już wyjaśnione ogólne zagadnienia możemy skupić się na danym zadaniu manipulatora. Załóżmy, że zadaniem robota jest pobranie kilku kostek z podajnika w miejscu A i przeniesienie ich w odpowiednie miejsca na palecie B, tzw. paletyzacja. Aby wykonać to zadanie możemy podejść do niego na dwa sposoby: zapewniając maszynie odpowiednią percepcję lub nie. 

\begin{itemize}
\item \textbf{Przypadek 1. Brak percepcji} \\
Brak percepcji oznacza brak danych wejściowych. Operujemy jedynie na wielkościach, które muszą zostać założone z góry. W tym przykładzie będą to: pozycje bazy i kiści manipulatora, podajnika, palety oraz rozmiar kostki. Z tego powodu, robot pozbawiony percepcji zawsze musi otrzymać od nas dokładne informacje odnośnie trajektorii ruchu.

\item \textbf{Przypadek 2. Percepcja z użyciem systemu wizyjnego} \\
Przez system wizyjny możemy rozumieć dowolny rodzaj kamery, jak również projektory chmur punktów i urządzenia nakładające na siebie kolory z danymi o głębokości obrazu, zwane kamerami RGBD. W takim przypadku operujemy znaczną ilością danych, dzięki czemu znajomość wymienionych wyżej wartości nie musi być konieczna - możemy oszacować te wartości używając metod widzenia maszynowego \emph{(Computer Vision).} Zastosowanie systemu wizyjnego możemy ponownie podzielić na dwa typy: z kamerą zamontowaną nieruchomo względem bazy robota oraz na jego narzędziu. Różnica między nimi polega głównie na tym, że w pierwszym przypadku widziane obiekty procesowane, jeśli robot ich bezpośrednio nie dotyka, pozostają w bezruchu względem kamery, a przez to nie zmienia się ich obraz. Przypadek z kamerą zamontowaną na kiści jest w stanie zapewnić więcej użytecznych ujęć, ale przez to jest też trudniejszy do zaimplementowania.
\end{itemize}

\begin{figure}[h]
\includegraphics[width=\textwidth]{basic}
\caption{Przykład zadania paletyzacji jako manipulacji przy braku percepcji. Po lewej widoczny jest kod programu stworzony w języku MELFA-BASIC IV. Po prawej na górze - ujęcie wykonane podczas pracy robota Kawasaki, na dole - widok palety do ukończeniu zadania. Symulacja została wykonana w środowisku Cosimir.}
\end{figure}

\begin{figure}[h]
\includegraphics[width=\textwidth]{bullet}
\caption{Przykład zadania \emph{bin picking} wykonywanego przez robota KUKA w oparciu o dane z systemu wizyjnego. Widoczne po lewej obrazy zostały wygenerowane sztucznie jako hipotetyczne dane z kamery umieszczonej nieruchomo względem bazy robota. Od góry - ujęcie powstałe jako bezpośrednia projekcja, obraz niosący dane o głębii i obraz poddany segmentacji. Symulacja została stworzona w języku Python z wykorzystaniem biblioteki \emph{pybullet}.}
\end{figure}

\subsection*{\LARGE{Cele manipulacji}} \addcontentsline{toc}{subsection}{Cele manipulacji}

Chociaż pierwsze w pełni sprawne manipulatory istnieją w sektorze industrialnym od dekad, wykorzystanie ich w zadaniu manipulacji stanowi wyzwanie, podczas gdy często stosowane rozwiązania nie wykorzystują w pełni ich potencjału. Dlatego chcąc określić potrzebę implementacji omawianych tu rozwiązań warto jest przyjrzeć się celom przed, którymi staje współczesna robotyka. Przede wszystkim chcemy, aby dane rozwiązanie było możliwie jak najbardziej \textbf{uniwersalne}, to znaczy działało poprawnie w różnych środowiskach, było odporne na zaszumienia i efektywne. Aby te cechy mogły zostać spełnione należy pewne kwestie muszą zostać rozwiązane, a należą do nich:

\begin{itemize}
\item \textbf{Zdolność do operowania wieloma różnymi obiektami.} Uniwersalność może zostać osiągnięta przez adaptację, czyli trwający w czasie proces, którego celem jest przystosowanie danego podmiotu do obserwowanego środowiska. Algorytmy działające zgodnie z tą zasadą stają się dość popularne szczególnie w ostatnich latach, a techniki wiodące prym na tym polu określane są mianem \emph{Reinforcement Learning} (w języku polskim przyjęły się nazwy uczenia ze wspomaganiem i uczenia z krytykiem). Nie zawsze stanowią jednak wydajne rozwiązanie zadania, głównie ze względu na wspomniany wyżej czas potrzebny do przetrenowania algorytmu. Problematyczne mogą być również dość chaotyczne, a przez to nieoptymalne i trudne do przewidzenia trajektorie jakie algorytmy tego typu generują.

\item \textbf{Sterowanie we wszystkich stopniach swobody}. Manipulator przemysłowy wykorzystany w tej pracy charakteryzuje się sześcioma stopniami swobody (bez chwytaka), co sprawia, że dla niektórych pozycji zadanie kinematyki odwrotnej będzie miało więcej niż jedno rozwiązanie i wiele możliwych trajektorii, z których nie wszystkie są optymalne.

\item \textbf{Odporność na zaszumione lub zniekształcone dane wejściowe}. Ograniczona rozdzielczość kamer RGBD, zaszumienie spowodowane światłem zewnętrznym i różne możliwe rozrzucenie obiektów komplikuje działanie algorytmu i aby proces przebiegał płynnie - musi zostać obsłużone.
\end{itemize}

Do powyższej listy moglibyśmy także dopisać zdolność do przenoszenia zachowań z symulacji do rzeczywistego świata, ale ponieważ nie we wszytkich problemach stosuje się metody symulacji - nie dopisywałem jej.

\subsection*{\LARGE{Afordancje}} \addcontentsline{toc}{subsection}{Afordancje}

Zdolność oddziaływania na jakiś obiekt lub środowisko została określona mianem \textbf{afordancji} przez psychologa Jamesa J. Gibsona w 1966 roku w książce \emph{The Senses Considered as Perceptual Systems} i opisana w artykule \emph{The  Theory  of  Affordance.  In:  Perceiving,  Acting  and  Knowing  Toward  an Ecological Psychology}. Definicja ta przyjęła szerokie znaczenie w dziedzinach tj. psychologia, kognitywistyka, sztuczna inteligencja czy robotyka. Możemy zdefiniować afordancje na przykładzie ludzi, ale człowiek ze względu na swoją złożoność często pozostaje niewdzięcznym modelem. Dlatego dla naszych rozważań posłużymy się morskim bezkręgowcem - krabem \emph{Limulus polyphemus}. Przykład ten został opisany m.in. w \emph{Biocybernetyce} Ryszarda Tadeusiewicza. Korzystnymi cechami tego stawonoga z naszego punktu widzenia są posiadanie ośrodkowego układu nerwowego oraz oczu, z których impulsy jesteśmy w stanie śledzić. Tak więc, jesteśmy w stanie w przybliżeniu zobrazować jego pole widzenia. 

Co wobec tego widzi krab? Przede wszystkim \textbf{obiekty} stanowiące najczęściej zagrożenie lub będące przedmiotem łowów niezależnie od tła. W warunkach laboratoryjnych możemy także osiągnąć pobudzenie neuronalne za pomocą punktowego oświetlenia lub gwałtownych zmian światła. Krab nie widzi natomiast równomiernych gradientów oświetlenia czy monotonnego pastelowego tła. Jest to biologicznie uzasadnione, ponieważ informacje te nie są dla niego w żaden sposób niezbędne do życia. Presja ewolucyjna wywierana na mózgi zwierząt kształtowała się z najbardziej podstawowych powodów: żeby umożliwić im lepszą zdolność poruszania się i odpowiednią zdolność postrzegania otoczenia. Te umiejętności znajdowały odzwierciedlenie w najbardziej fundamentalnych cechach życiowych, czyli poszukiwaniu pożywienia i schronienia oraz ucieczki przed zagrożeniem. Dlatego jeśli prześledzimy drogę ewolucji prowadzącą do prostych bezkręgowców tj. glisty czy małże, zauważymy, że ich ośrodkowy układ nerwowy odpowiada głównie za kontrolę ruchu przez pobudzanie właściwych mięśni. A ponieważ nawet u najbardziej prymitywnych zwierząt ruch wykształcał się w parze ze zdolnością postrzegania środowiska - receptory reagujące na sygnały zewnętrzne (substancje chemiczne lub światło) przesyłają impulsy elektryczne do nerwów odpowiedzialnych za poruszanie się. 

Przykład kraba, mimo że może wydawać się surowy, niesie ze sobą istotne wskazówki na temat tego co powinniśmy rozumieć przez afordancje i jaka jest ich rola. Nikt dokładnie nie wie jak wyglądała droga rozwoju układu nerwowego prowadząca do zorganizowania jego struktury w mózg dorosłego człowieka, w którym ponad połowa neuronów odpowiada za kontrolę motoryczną i zmysły. Aby mieć władzę nad ciałem, mózg tworzy pewne odwzorowanie, które "mapuje" poszczególne części ciała na powierzchni kory mózgowej w miejscu zwojów \emph{gyrus postcentralis} (pól Brodmanna 1, 2 i 3). Mając taką mapę możemy zauważyć, że łączna powierzchnia dłoni i twarzy zajmuje na niej znacznie więcej miejsca niż pozostała reszta ciała. Innymi słowy, czynności wykonywane twarzą i dłońmi, tj. mimika, mowa czy manipulacja, angażują znaczne ilości neuronów co stanowi pewną miarę ich skomplikowania. Ostatnie badania stosujące techniki neuroobrazowania sugerują, że umiejętność "obchodzenia się z narzędziami", czyli zdolność do projektowania, planowania i korzystania z narzędzi wyewoluowała w lewej półkuli mózgu. Dlatego też pacjenci, u których zdiagnozowano uszkodzenia tej części, mimo że potrafią rozpoznać dany przedmiot, nie potrafią sobie wyobrazić jak go użyć i nie radzą sobie w tej kwestii lepiej niż szympansy. 

Tak więc, aby mogło zostać zrealizowane chwytanie -  wykorzystywane są informacje zawarte w obrazie. Pozwala to tłumaczyć dlaczego niektóre gatunki, przykładem tu mogą być wczesne homidy, wykształciły bardziej rozwinięty wzrok konieczny do wytypowania elementów otoczenia. Podobnie w robotyce, robot musi nauczyć się jak chwytać i operować uchwyconymi obiektami, tak aby osiągnąć wyznaczony cel. Różne przedmioty, np. młotek mogą być uchwycone na wiele różnych sposobów natomiast liczba optymalnych chwytów jest ograniczona w kontekście danego zadania. 

\subsection*{\LARGE{Zarys pracy}} \addcontentsline{toc}{subsection}{Zarys pracy}

Mając wprowadzenie za sobą możemy skupić się na wymienieniu kolejnych etapów pracy. Głównym jej dążeniem jest znalezienie znanego przedmiotu i uchwycenie go za pomocą manipulatora. Aby to osiągnąć najpierw możemy rozbić ten problem na mniejsze, do których należą estymacja położenia i orientacji obiektu, zadanie kinematyki odwrotnej oraz chwytanie.

\begin{itemize}
\item \textbf{Estymacja położenia i orientacji obiektu} \\
Do rozwiązania tego zadania zostanie zaproponowana metoda inteligencji obliczeniowej znana pod nazwą ICP (ang. \emph{Iterative Closest Point}). W oparciu o model obiektu znajduje ona trnasformację jakiej ten model trzeba poddać, aby znalazł się w miejscu szukanego przedmiotu. Jej szczegółowe działanie zostanie szerzej opisane w kolejnych rozdziałach.

\item \textbf{Zadanie kinematyki odwrotnej} \\
Problem znalezienia odpowiednich wielkości opisujących manipulator jako łańcuch kinematyczny dzielimy na dwa mniejsze: zadanie kinematyki prostej i odwrotnej. Proste zadanie kinematyki odpowiada na pytanie w jakim miejscu znajduje się efektor końcowy dla danej pozy manipulatora. Natomiast kinematyka odwrotna zakłada, że znane jest docelowe położenie efektora końcowego a szukana jest pozycja (lub pozycje) robota, które pozwolą mu je osiągnąć, dlatego też zostanie wykorzystana w tej pracy. Mając na uwadze, że większość robotów przemysłowych posiada wbudowaną implementację kinematyki odwrotnej nie będziemy wchodzić w jej szczegóły.

\item \textbf{Chwytanie} \\
Do realizacji chwytania posłużymy się chwytakiem, którego orientacja wobec chwytanego przedmiotu zostanie dobrana z góry. Stanowić to może pewien mankament, ponieważ działanie uzależniane jest tym samym od ingerencji człowieka. Z drugiej strony, daje to większe prawdopodobieństwo pewniejszego i bardziej stabilnego chwytu, który może okazać się kluczowy w praktycznych zastosowaniach.
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=15cm]{catch}
\caption{Ręczne zorientowanie szczęk chwytaka robota UR5 wobec obiektu. Wizualizacja została wykonana na potrzeby pracy z użyciem biblioteki \emph{pybullet}.}
\end{figure}

Zanim przejdziemy do przedstawienia proponowanego rozwiązania przez rozwinięcie powyższych kwestii zostanie zrobiony przegląd najważniejszych obecnie stosowanych metod. Następnie będzie miał miejsce teoretyczny opis pewnych zagadnień. Zostaną pokazane matematyczne podstawy algorytmu \emph{Iterative Closest Point} i jego implementacja. Pod koniec zostanie zrobione podsumowanie, podczas którego będą wyciągnięte wnioski odnośnie działania proponowanego rozwiązania oraz możliwości dalszego rozwoju.

\begin{figure}[h]
\centering
\includegraphics[width=15cm]{workbench}
\caption{Stanowisko z robotem UR5. Na niebiesko jest widoczna umieszczona pod kątem kamera \emph{Intel RealSense} skierowana na stół.}
\end{figure}

\newpage
\section*{\LARGE{2. Przegląd dotychczasowych rozwiązań}} \addcontentsline{toc}{section}{2. Przegląd dotychczasowych rozwiązań}

Problem sterowania robotami z wykorzystaniem informacji zwrotnej z otoczenia sięga początków współczesnej robotyki. Chcąc zapewnić możliwość pracy robota w środowiskach, w których nie mamy wystarczającej wiedzy na temat położenia obiektów z jakimi ma wchodzić w interakcje, konieczne jest zaimplementowanie podstawowej percepcji. Najprostszy przypadek może stanowić robot wyposażony w czujnik koloru w zadaniu sortowania kolorowych kulek. Wiele rozwiązań jednak opiera się na wykorzystaniu danych i przetworzeniu obrazu z kamery i czujników w taki sposób, aby robot był w stanie rozpoznać lub zlokalizować wskazane obiekty. W tym rozdziale przyjrzymy się obecnie stosowanym rozwiązaniom, które wpisują się w popularne trendy na tym polu. Aby wprowadzić pewien porządek, wyodrębnione zostały trzy najbardziej istotne grupy: widzenie maszynowe, sieci neuronowe oraz uczenie przez wzmacnianie. Ponieważ są to tematy dosyć obszerne, uwaga zostanie skupiona na podstawowych zasadach ich działania bez szczególnego wchodzenia obliczenia i metody matematyczne, które za nimi stoją - takie rozważania wychodziły poza temat tej pracy. Nabycie takiej intuicji okaże się pomocne przy późniejszej ocenie proponowanego rozwiązania.

\subsection*{\LARGE{Widzenie maszynowe}} \addcontentsline{toc}{subsection}{Widzenie maszynowe}

Analizę i przetwarzanie obrazu nazywany widzeniem maszynowym lub krócej CV od angielskich słów \emph{Computer Vision}. Stało się ono popularne szczególnie na początku lat '00 wraz ze wzrostem wydajności obliczeniowej ówczesnych komputerów. Wyróżniamy kilka typów problemów związanych z obrazami, takich jak: detekcja, lokalizacja i segmentacja.

\begin{itemize}
\item \textbf{Detekcja} dostarcza nam informację czy wskazany obiekt znajduje się na obrazie,
a także w niektórych przypadkach, otrzymujemy prawdopodobieństwo jego wystąpienia. Nie zapewnia nam jednak informacji, gdzie dokładnie ten obiekt się znajduje.

\item \textbf{Lokalizacja} często pojawia się w parze z detekcją. Wynika to z faktu, że gdy mamy rozpoznany dany obiekt, chcemy pozyskać także informację o jego położeniu, co otrzymujemy w postaci współrzędnych lokalizacji, a czasem także wielkości.

\item \textbf{Semantyczna segmentacja} niesie nam odpowiedź o położeniu oraz kształcie obiektów. Załóżmy, że mamy trzy różne obiekty w puli do rozpoznania i wszystkie znajdują się na jednym obrazie, z czego dwa z nich są na nim po jednej sztuce, a ostatni – w dwóch. Semantyczna segmentacja umożliwi nam znalezienie położenia ich wszystkich, natomiast nadal nie wiemy czy jakiś z nich nie wystąpił w więcej niż jednej sztuce, przez co niemożliwe jest określenie ilości.

\item \textbf{Segmentacja z uwzględnieniem instancji} powstała jako rozwiązanie problemu opisanego powyżej. Niesie nam odpowiedź, która oprócz podania dokładnego położenia i kształtu obiektów także uwzględnia ich ilość.
\end{itemize} 

Wymieniowe wyżej problemy należą do dość złożonych zadań wymagających sporej mocy obliczeniowej. Z pomocą przychodzą programy takie jak SIFT i algorytm Violi-Jonesa (w celu detekcji i lokalizacji danych obiektów na podstawie podobieństwa cech) czy metody uczenia nienadzorowanego (pomocne przy segmentacji). Często jednak stosowane są sztuczne sieci neuronowe, a szczególnie popularne są sieci zawierające operację splotu, czyli sieci splotowe.

\subsection*{\LARGE{Sztuczne sieci neuronowe}} \addcontentsline{toc}{subsection}{Sztuczne sieci neuronowe}

Mówiąc o sieciach neuronowych w kontekście przedmiotów zajmujących się przetwarzaniem sygnałów mamy na myśli struktury matematyczne, które w pewnym stopniu przypominają budowę i działanie ludzkiego mózgu. Są one jedną z najbardziej popularnych technik współczesnej sztucznej inteligencji. Stosujemy je przede wszystkim wtedy, gdy nie znamy reguł rozwiązania danego problemu, ale dysponujemy zbiorem przykładowych zadań (zbiorem uczącym), które zostały poprawnie rozwiązane. 

Sztuczne sieci neuronowe składają się z wzajemnie połączonych neuronów, które są uproszczonymi modelami biologicznych odpowiedników. Jest to spowodowane tym, że rzeczywiste neurony są tworami niezmiernie skomplikowanymi i wymagającymi pod względem obliczeniowym. Gdy przedstawimy taką sieć w postaci grafu - jego wierzchołki będą reprezentować neurony, a krawędzie - poszczególne wagi sieci. Jej trening polega na znalezieniu takich wartości wag, które zapewnią poprawne rozwiązanie problemu. Zazwyczaj odbywa się to za pomocą algorytmu wstecznej propagacji błędów (ang. \emph{backpropagation}). Trenując sieć, generalizuje ona wiedzę na podstawie zbioru uczącego, od którego zależy jej późniejsze działanie.

\subsubsection*{\Large{Sieci jednokierunkowe}}
Sztandarowym przykładem jest tu wielowarstwowa sieć jednokierunkowa, zwana również perceptronem wielowarstwowym lub MLP (ang. \emph{Multi-Layer Perceptron}). Jak sama nazwa wskazuje jest to struktura, w której zostały wyodrębnione \textbf{warstwy}, z których pierwszą nazywamy wejściową, środkowe - ukrytymi, a ostatnią - wyjściową. Warstwy te reprezentują kolejne poziomy abstrakcji tworząc model hierarchiczny. Najniższe z nich wykrywają proste cechy sygnału wejściowego, a najwyższe, opierając się o informacje z poprzednich warstw - są w stanie znaleźć pewne coraz bardziej abstrakcyjne cechy. 

Posługując się przykładem, załóżmy, że chcemy przetrenować pięciowarstwową sieć pod kątem zdolności do rozpoznawania obrazu. Przechodząc przez warstwy ukryte w najbardziej powszechnym przypadku pierwsza z nich wykryje krawędzie na podstawie różnic kolorów między pikselami, druga - korzystając z informacji o krawędziach będzie w stanie znaleźć kontury i zaokrąglenia, trzecia - pewne relacje pomiędzy nimi. Na ich podstawie z większym prawdopodobieństwem będzie mogła stwierdzić do jakiej klasy należy widziany obraz. W ten sposób, najczęściej w postaci rozkładu prawdopodobieństwa, dostaniemy informację o rozpoznanym obiekcie.

\begin{figure}[h!]
\centering
\includegraphics[width=15cm]{neural_net}
\caption{Idea działania sieci neuronowej w zadaniu rozpoznawania obiektu. Przedstawiona została sieć składająca się z pięciu warstw: wejściowej, trzech ukrytych i wyjściowej. Na wejście trafiają dane, np. obraz lub chmura punktów, którą chcemy rozpoznać. Następnie kolejne warstwy ukryte wydobywają z nich coraz bardziej abstrakcyjne cechy. Pod koniec, informacja o przynależności do danej klasy A, B lub C jest zwracana na wyjściu sieci. Obraz został zaczerpnięty z książki \emph{Deep Learning: Systemy uczące się}.}
\end{figure}

\subsubsection*{\Large{Sieci splotowe}}
Rodzaj sieci, na który szczególnie warto jest zwrócić uwagę stanowią sieci splotowe zwane częściej sieciami konwolucyjnymi, w skrócie CNN (ang. \emph{Convolutional Neural Network}). Te sieci zawdzięczają swoją popularność przede wszystkim dzięki wykorzystaniu do problemów związanych z analizą obrazów, przy których ich możliwości przewyższają zwykłe sieci jednokierunkowe. Ich nazwa wzięła się od operacji splotu (konwolucji), która jest wykorzystywana do ekstrakcji cech przez splot obrazu z filtrem. Sieci splotowe są z powodzeniem stosowane w rozpoznawaniu, klasyfikacji i segmentacji obrazów. Ponieważ możliwe jest przeprowadzenie operacji splotu na danych o zarówno jednym, dwóch oraz trzech wymiarach, użycie sieci CNN da się poszerzyć do analizy danych takich jak dźwięk, obrazy RGBD czy chmury punktów, a to z kolei implikuje ich przydatność w robotyce.

\subsubsection*{\Large{GraspNet}}
Daleko idącym przykładem wykorzystania sieci neuronowych w robotyce jest projekt \emph{GraspNet}. Celem przedsięwzięcia było przetrenowanie modelu sztucznej inteligencji na zbiorze obrazów pochodzących z kamer RGBD przedstawiających przedmioty z możliwymi chwytami wyznaczonymi analitycznie. Twórcom projektu udało się zgromadzić 97280 obrazów RGBD zawierających ponad miliard możliwych pozycji chwytu. Zbiór ten został ujawniony i opublikowany pod nazwą \emph{GraspNet-1Billion}. W rzeczywistości \emph{GraspNet} jest modelem składającym się z kilku mniejszych sieci, który oprócz nich wykorzystuje funkcje tj. grupowanie, wyśrodkowywanie i filtrowanie danych. Tak więc, dla danych wejściowych w postaci chmury punktów przyporządkowuje pewną ilość chwytów jakie mogą zostać zastosowane podczas próby uchwycenia obiektów w niej przedstawionych. Możliwe chwyty zostają wyznaczone w sposób, na który mogą zostać przeprowadzone z użyciem chwytaka dwupalczastego.

\begin{figure}[h]
\centering
\includegraphics[width=10cm]{graspnet}
\caption{Wizualizacja możliwych chwytów wykrytych przez sieć \emph{GraspNet}. Na niebiesko oznaczone jest położenie szczęk chwytaka.}
\end{figure}

\subsubsection*{\Large{Zalety sieci neuronowych}}
Oprócz zdolności do rozwiązywania problemów, które nie są efektywnie algorytmizowalne w oparciu o matematyczne modelowanie wiedzy, do zalet sieci neuronowych możemy zaliczyć ich wszechstronność. To znaczy, dana struktura sieci może zostać przetrenowana do wielu różnych niezwiązanych ze sobą zadań. Jest to w pewnym sensie cecha wspólna wszystkich technik uczenia maszynowego wyróżniająca je na tle innych algorytmów.

\subsubsection*{\Large{Wady sieci neuronowych}}
Sieci neuronowe mają wiele wad - trenowane algorytmem \emph{backpropagation} dokonują coraz mniejszych zmian w kolejnych warstwach idących wgłąb sieci (problem zanikającego gradientu) oraz łatwo przeuczają się, ponieważ większa liczba zmieniających się podczas uczenia sieci parametrów sprzyja przetrenowaniu. Skutkiem tego jest mniejsza wydajność proces ekstrakcji cech występujących w zbiorze uczącym. Trenując sieci neuronowe, w szczególności duże modele, nie do końca wiemy w jaki sposób przetwarzają one informacje. Pociąga to za sobą wiele implikacji, a patrząc z technicznej perspektywy, często nie jesteśmy pewni czy zastosowany przez nas model sieci nie mógłby zostać uproszczony. Gdybyśmy dysponowali większą wiedzą na temat przetwarzania przez nie informacji, najprawdopodobniej możliwa była by redukcja czasu, danych i zasobów obliczeniowych potrzebnych do ich uczenia. Są to najbardziej istotne czynniki wpływające na jakość działania modelu.

\subsection*{\LARGE{Uczenie przez wzmacnianie}} \addcontentsline{toc}{subsection}{Uczenie przez wzmacnianie}

Uczenie przez wzmacnianie zwane też uczeniem z krytykiem lub \emph{Reinforcement Learning} opisuje metody uczenia maszynowego, w którym optymalne rozwiązanie danego zadania powstaje na skutek interakcji agenta ze środowiskiem. W zastosowaniach tego podejścia w robotyce, środowiskiem jest symulacja lub rzeczywiste otoczenie, z którym agent (robot) wchodzi w interakcję. Celem jest maksymalizacja zysku ściśle związanego z pożądanym rezultatem. W ten sposób robot próbując osiągnąć jak najwyższy wynik uczy się rozwiązywać zadany problem.

\begin{figure}[h]
\centering
\includegraphics[width=10cm]{rl}
\caption{Przedstawienie zasady działania metod \emph{Reinforcement Learning} w postaci grafu: agent podejmuje działanie w danym środowisku co jest interpretowane jako zysk i reprezentacja stanu, które trafiają z powrotem do agenta. W dalej idących implementacjach algorytmu występują także obserwacje, czyli dostarczane agentowi informacje na temat środowiska.}
\end{figure}

Istnieje wiele wariantów algorytmów wykorzystujących \emph{Reinforcement Learning}, które możemy podzielić ze względu na kryterium optymalizacji na oparte na polityce lub funkcji zmiany stanu.

\subsubsection*{\Large{Oparte na polityce}}
W tym podejściu przyjmowana jest polityka, którą należy zoptymalizować. Politykę możemy definiować jako odpowiedzialną za zachowanie agenta funkcję, która na wejściu przyjmuje obserwację, a na wyjściu zwraca akcję.  

\subsubsection*{\Large{Oparte na funkcji wartości stanu}}
Funkcja wartości stanu $V_{\pi}(s)$ określa jak wysoki będzie zysk, gdy zaczniemy ze stanu $s$ i będziemy opierać się o politykę $\pi$. 

\[V_{\pi}(s) = \mathbb{E}[R | s_0=s] = \mathbb{E} \left[\sum_{t=0}^n \gamma^t r_t | s_0=s \right]\]

W tym kryterium zostaje wprowadzona wartość $R$ jako całkowity \textbf{zysk}, który jest zdefiniowany jako suma poszczególnych przyszłych zysków

\[R = r_{0} + \gamma r_1 + \gamma^2 r_2 + \ldots + \gamma^n r_n = \sum_{t=0}^n \gamma^t r_t \quad gdzie \quad \gamma \in \langle 0, 1)\]

Ponieważ $\gamma$ jest mniejsza niż 1, odległe przyszłe zdarzenia są mniej istotne od zdarzeń w bliskiej i niedalekiej przyszłości. Jest to istotne, ponieważ zależy nam na najszybszym możliwym uzyskaniu zysku.

\subsubsection*{\Large{Deep Reinforcement Learning}}
Związek uczenia przez wzmacnianie z robotyką stale się zacieśnia, zwłaszcza popularne w ostatnich latach stały się techniki wykorzystujące głębokie uczenie przez wzmacnianie, czyli \emph{Deep Reinforcement Learning}. Jest to w pewnym sensie połączenie dwóch omawianych wyżej metod: sieci neuronowych i \emph{Reinforcement Learningu} stosowane w sytuacjach, gdy mamy do czynienia z bardziej złożonymi problemami. Posługując się konkretnym przykładem, załóżmy że chcemy, aby robot na podstawie obrazu chmury punktów pochodzącej z kamery RGBD znalazł zadany przedmiot i go uchwycił. Jest to problem, na którym skupia się ta praca, szerzej znany pod nazwą \emph{bin picking}. Wówczas, gdy mamy wielowymiarowe obserwacje w postaci chmury punktów - niemożliwe się staje rozwiązanie go za pomocą tradycyjnego \emph{Reinforcement Learningu}. 

\begin{figure}[h]
\centering
\includegraphics{rubik}
\caption{Robot wykorzystujący techniki uczenia ze wzmacnianiem podczas układania kostki Rubika. Eksperyment został opisany w publikacji \emph{Solving Rubik's Cube with a Robot Hand} i miał na celu przetrenowanie sztucznej inteligencji pod kątem umiejętności motorycznych - praca zbiorowa OpenAI.}
\end{figure}

\subsubsection*{\Large{Zalety uczenia przez wzmacnianie}}
Do niewątpliwych zalet uczenia przez wzmacnianie należy zdolność do tworzenia zachowań \textbf{emergentnych}. Przez emergencję rozumiemy powstawanie zachowań, do których instrukcje nie zostały bezpośrednio udzielone, a znalezione metodą prób i błędów i następnie z powodzeniem stosowane. Drugą ważną zaletą jest jego uniwersalność - metody oparte na \emph{Reinforcement Learning} mogą zostać z lepszym lub gorszym skutkiem użyte w większości problemów związanych z motoryką w robotyce, tj. nauka chodzenia, chwytania itp.

\subsubsection*{\Large{Wady uczenia przez wzmacnianie}}
Jak już zostało wspomniane we wprowadzeniu, do wad podejścia wykorzystującego uczenie przez wzmacnianie w robotyce zaliczamy czas i często zasoby obliczeniowe potrzebne do przetrenowania algorytmu oraz niewydajne i często chaotyczne ruchy, które są jego rezultatem.

\newpage
\section*{\LARGE{3. Estymacja położenia obiektu}} \addcontentsline{toc}{section}{3. Estymacja położenia obiektu}

Aby przejść do zadania estymacji położenia obiektu najpierw należy określić czy dysponujemy jego kształtem. Ponieważ w wielu zastosowaniach industrialnych przedstawionego algorytmu cecha ta jest wiadoma z góry, dla potrzeby tej pracy możemy przyjąć, że posiadamy taki model. Mamy więc podane dwa kształty: rzeczywisty model obiektu pobrany z kamery RGBD jako chmura punktów oraz wyidealizowany model, który posłuży nam za szablon. Oba są reprezentowane jako zbiory punktów, z których każdy opisany jest przez współrzędne x, y, z. W tym rozdziale omówiony szerzej zostanie problem estymacji położenia, zanim jednak do tego przejdziemy skupimy się na podstawowych założeniach i sposobach, które do niego prowadzą. Zostaną opisane także konstrukcje matematyczne, tj. elementarne macierze transformacji, kwaterniony czy rozkład według wartości osobliwych, które pozwolą lepiej zrozumieć temat przewodni pracy.

\begin{figure}[h]
\centering
\includegraphics[width=10cm]{bunny}
\caption{Model obiektu i utworzona sztucznie chmura punktów.}
\end{figure}

\subsection*{\LARGE{Wyznaczenie najbliższego punktu}} \addcontentsline{toc}{subsection}{Wyznaczenie najbliższego punktu}

Najbardziej fundamentalny przykład sprowadza się do znalezienia długości odcinka łączącego dwa punkty $\vb{p}_{1} = (x_{1}, y_{1}, z_{1})$ i $\vb{p}_{2} = (x_{2}, y_{2}, z_{2})$. Odległość ta jest dana w metryce euklidesowej wzorem
\[d(\vb{p}_{1}, \vb{p}_{2}) = ||\vb{p}_{1} - \vb{p}_{2}|| = \sqrt{(x_{2}-x_{1})^2+(y_{2}-y_{1})^2+(z_{2}-z_{1})^2}\]

Kolejnym krokiem ku estymacji położenia jednego obiektu względem drugiego wydaje się być przeprowadzenie tej operacji dla wszystkich punktów. W tym miejscu nasuwa się jednak pewien problem. Mianowicie, nie tylko nie wiemy czy dany punkt ma swojego odpowiednika, ale też czy ich liczba w obu zbiorach się zgadza. W rzeczywistości próbujemy znaleźć najkrótszą drogę między punktami zbioru $\mathbf{A}$ a punktami zbioru $\mathbf{B}$ do czego posłuży algorytm najbliższego sąsiada.

\subsection*{\LARGE{Algorytm najbliższego sąsiada}} \addcontentsline{toc}{subsection}{Algorytm najbliższego sąsiada}

Algorytm najbliższego sąsiada (ang. \emph{Nearest Neighbour algorithm}) jest jedną z najbardziej podstawowych technik wykorzystywaną do rozpoznawania wzorców we współczesnym uczeniu maszynowym. Jest to algorytm zachłanny o złożoności czasowej $O(n^2)$ o czym świadczy zagnieżdżenie dwóch pętli. Może zostać wykorzystany zarówno do klasyfikacji, regresji jak i do rozwiązania problemu komiwojażera. 

\begin{comment}
Chcąc znaleźć najlepsze dopasowanie między dwoma zbiorami punktów potrzebujemy miary tego dopasowania. W tym celu posługujemy sumą odległości korespondujących punktów w metryce euklidesowej. Jeśli przyjmiemy, że po pomiarze odległości danego punktu - staje się on odwiedzony, taką metodę możemy rozpatrywać posługując się terminologią typową dla teorii grafów. Podany algorytm da się rozpisać w postaci listy kroków.

\begin{enumerate}
\item Ustawienie statutu wszystkich punktów jako nieodwiedzone.
\item Wybranie jednego z nich i zmierzenie odległości między nim a punktem $\vb{p}$.
\item Znalezienie najkrótszej odległości między 
\end{enumerate}
\end{comment}

\vspace{5mm}
\lstinputlisting[language=Python]{knn.py}
\vspace{5mm}

Opisując powyższy kod, dana jest funkcja \emph{knn} przyjmująca trzy argumenty: zbiór punktów w postaci listy, punkt $p$ oraz liczbę najbliższych sąsiadów domyślnie ustawioną na jeden. Działanie funkcji zaczyna się od zdefiniowania pustej listy, w której będą przechowywane poszczególne odległości. Poniżej znajduje się zagnieżdżenie dwóch pętli, z których pierwsza iteruje po punktach, a druga - po ich współrzędnych. W linijkach 6-7 liczona jest odległość euklidesowa między danym punktem w zbiorze, a punktem $p$, która następnie zapisywana jest do listy $distances$. Po wyjściu z obu pętli, lista ta jest sortowana i zwracana na wyjściu funkcji.

\subsection*{\LARGE{Transformacje}} \addcontentsline{toc}{subsection}{Transformacje}

Mianem transformacji określana jest funkcja przekształcająca jeden obiekt w drugi. Ze względu, że przez cały czas trwania tej pracy będziemy rozważać przedmioty jako bryły sztywne - omówione zostaną jedynie transformacje, które nie zmieniają kształtu ani rozmiaru obiektu. 
\[
\mathbf{R} = \begin{bmatrix}
R_{xx} & R_{xy} & R_{xz} \\
R_{yx} & R_{yy} & R_{yz} \\
R_{zx} & R_{zy} & R_{zz}
\end{bmatrix}
\qquad
\vb{t}^\top = \begin{bmatrix}
t_{x} \\ t_{y} \\ t_{z}
\end{bmatrix}
\]

\subsubsection*{\Large{Macierz rotacji}}
W zależności od tego jakim sposobem chcemy wykonywać działania mamy do wyboru trzy równoważne opcje obrotu danego kształtu w przestrzeni. Możemy w tym celu posłużyć się macierzą opartą o kąty obrotu wokół wszystkich trzech osi w układzie kartezjańskim, macierzą opartą o oś obrotu i kąt o jaki dany obiekt ma zostać obrócony lub kwaternionem. Dla zadanej rotacji wszystkie trzy sposoby z większą lub mniejszą dokładnością zwrócą numerycznie te same wyniki.

\subsubsection*{\Large{Elementarne macierze rotacji}}
Najprostszym sposobem na obrócenie obiektu w przestrzeni trójwymiarowej jest skorzystanie z elementarnych macierzy transformacji. Jest to podejście, którego używamy gdy dane są trzy kąty położone na prostopadłych płaszczyznach, wokół których należy obrócić obiekt. katy Eulera. Czasami można spotkać się także z nazwami \emph{pitch-roll-yaw} oznaczającymi to samo. Obrót o każdy taki kąt możemy zapisać w postaci macierzy
\[
\mathbf{R}_z(\gamma) = \begin{bmatrix}
\cos \gamma & -\sin \gamma & 0 \\
\sin \gamma & \cos \gamma & 0 \\
0 & 0 & 1
\end{bmatrix}
\]
\[
\mathbf{R}_y(\beta) = \begin{bmatrix}
\cos \beta & 0 & \sin \beta \\
0 & 1 & 0 \\
-\sin \beta & 0 & \cos \beta
\end{bmatrix}
\]
\[
\mathbf{R}_x(\alpha) = \begin{bmatrix}
1 & 0 & 0 \\
0 & \cos \alpha & -\sin \alpha \\
0 & \sin \alpha & \cos \alpha \\
\end{bmatrix}
\]
Gdy chcemy dokonać rotacji wokół więcej niż jednej osi składamy poszczególne obroty mnożąc je przez siebie. Mamy więc trzy obroty, które składają się na wynikową rotację.
\[\mathbf{R} = \mathbf{R}_z(\gamma)\mathbf{R}_y(\beta)\mathbf{R}_x(\alpha)\]

Po przemnożeniu przez siebie wszystkich trzech obrotów składowych macierz rotacji przyjmuje postać
\[
\mathbf{R} = \begin{bmatrix}
\cos \gamma \cos \beta & 
\cos \gamma \sin \beta \sin \alpha - \sin \gamma \cos \alpha &
\cos \gamma \sin \beta \cos \alpha + \sin \gamma \sin \alpha \\
\sin \gamma \cos \beta &
\sin \gamma \sin \beta \sin \alpha + \cos \gamma \cos \alpha &
\sin \gamma \sin \beta \cos \alpha - \cos \gamma \sin \alpha \\
-\sin \beta & \cos \beta \sin \alpha & \cos \beta \cos \alpha
\end{bmatrix}
\]

\subsubsection*{\Large{Oś obrotu i kąt}}
Podczas rozpisywania złożonych transformacji za pomocą elementarnych macierzy rotacji możemy zauważyć, że w rzeczywistości zamiast obracać obiekt trzykrotnie wokół różnych osi - moglibyśmy tak dobrać jedną oś i kąt obrotu wokół niej, aby wynikowa rotacja pozostała tożsama. Dlatego w tym podejściu zakładamy, że dysponujemy taką osią i wykonujemy już tylko jeden obrót. Oś obrotu możemy przedstawić w postaci wektora jednostkowego $\textbf{u} = [u_x, u_y, u_z]$ takiego, że 
\[u_x^2+u_y^2+u_z^2 = 1 \]
Dla ułatwienia oznaczmy $c = \cos \theta$ i $s = \sin \theta$
\[
\mathbf{R} = \begin{bmatrix}
c+u_{x}^2(1-c) & u_{x}u_{y}(1-c)-u_{z}s & u_{x}u_{z}(1-c)+u_{y}s \\
u_{y}u_{x}(1-c)-u_{z}s & c+u_{y}^2(1-c) & u_{y}u_{z}(1-c)+u_{x}s \\
u_{z}u_{x}(1-c)-u_{z}s & u_{z}u_{y}(1-c)-u_{x}s & c+u_{z}^2(1-c)
\end{bmatrix}
\]

\subsubsection*{\Large{Kwaternion}}
Podejście wykorzystujące kwaternion różni się od dwóch powyższych głównie tym, że nie są tu używane funkcje trygonometryczne. Jest też pod pewnym względem podobne do przypadku wykorzystującego os obrotu i kąt, ponieważ ich obu nie da się rozbić na mniejsze obroty składowe. Same kwaterniony są przykładem liczb hiperzespolonych z jedną wartością rzeczywistą i trzema urojonymi. Kwaternion możemy zapisać w postaci sumy algebraicznej jako
\[q = a \cdot \textbf{e} + b \cdot \textbf{i} + c \cdot \textbf{j} + d \cdot \textbf{k} \qquad gdzie \quad a, b, c, d \in \mathbb{R} \]

Zaś $\textbf{e, i, j, k}$ to pewne jednostki urojone, między którymi zachodzi zależność 
\[i^2 = j^2 = k^2 = -1 \]

Wówczas transformację tą możemy zapisać równoważnie w postaci macierzy
\[
\mathbf{R} = \begin{bmatrix}
a^2+b^2-c^2-d^2 & 2(bc-ad) & 2(bd+ac) \\
2(bc+ad) & a^2+c^2-b^2-d^2 & 2(cd-ab) \\
2(bd-ac) & 2(cd + ab) & a^2+d^2-b^2-c^2
\end{bmatrix}
\]

\subsection*{\LARGE{Rozkład macierzy według wartości osobliwych}} \addcontentsline{toc}{subsection}{Rozkład macierzy według wartości osobliwych}

Celem rozkładu według wartości osobliwych, w skrócie rozkładu SVD (ang. \emph{Singular Value Decomposition}) w proponowanym algorytmie jest znalezienie macierzy rotacji, która stanowi transformację między chmurą punktów a szablonem, do którego próbujemy ją dopasować. Umożliwi to późniejsze uchwycenie przedmiotu przez chwytak manipulatora, ponieważ szablon jest obiektem, który jest znany, a więc możemy zamodelować poprawny proces chwytania.

Opisany w ten sposób problem stanowi jedynie pewien szczególny przypadek zastosowania rozkładu SVD. Ogólnie rzecz biorąc, SVD jest metodą pozwalającą na znalezienie rzędu macierzy, co teoretycznie może zostać wyznaczone metodą eliminacji Gaussa. To podejście jest jednak niepraktyczne, gdy ma zostać zrealizowane za pomocą metod numerycznych. Błędy biorą się często z niedoszacowań z powodu zaokrąglania wartości. Na przykład gdy rząd macierzy $\mathbf{A}$ jest niedoszacowany a $\mathbf{U}$ stanowi obliczoną numerycznie postać schodkową, wówczas możliwe jest, że $\mathbf{U}$ będzie miało niepoprawną liczbę niezerowych wierszy.

Najprostszym sposobem na zrozumienie rozkładu według wartości osobliwych jest zapisanie macierzy rzeczywistej $\mathbf{A}$ w postaci iloczynu trzech czynników, z których środkowy to macierz z pewnymi wartościami rosnącymi wzdłuż przekątnej. Taki rozkład jest zawsze możliwy, co można zapisać jako
\[\mathbf{A = UDV^\top} \]

Zakładając, że $\mathbf{A}$ jest macierzą $m \times n$, wówczas $\mathbf{U}$ stanie się macierzą ortogonalną o wymiarach $m \times m$, $\mathbf{V}$ - macierzą ortogonalną o wymiarach $n \times n$, a $\mathbf{D}$ macierzą $m \times n$, której wartości nieleżące na przekątnej są równe 0, a wartości na przekątnej spełniają zależność
\[\sigma_1 \geq \sigma_2 \geq \ldots \geq \sigma_n \geq 0\]

\[
\mathbf{D} = \begin{bmatrix}
\sigma_1 & & & \\
& \sigma_2 & & \\
& & \ddots & \\
& & & \sigma_n \\\\
\end{bmatrix}
\]
Wartości $\sigma_i$ określone przez rozkład są różne i są nazywane \textbf{wartościami osobliwymi} macierzy $\mathbf{A}$. Liczba niezerowych wartości osobliwych jest równa rzędowi macierzy $\mathbf{A}$, a ich wielkość stanowi miarę jak bardzo ta macierz różni się od macierzy niższego rzędu. Z kolei kolumny macierzy $\mathbf{U}$ stanowią \textbf{lewostronne wektory osobliwe} a kolumny $\mathbf{V}$ - \textbf{prawostronne wektory osobliwe}.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{five_svd}
\caption{Graficzne przedstawienie zasady działania SVD na przykładzie obrazu ręcznie pisanej cyfry pięć o wymiarach 28 $\times$ 28 pikseli. Kolory zbliżone do fioletu reprezentują niskie wartości, a jaskrawe zbliżone ku żółci - wysokie. Możemy zauważyć, że sposób w jakim na obrazie macierzy $\mathbf{D}$ układają się coraz ciemniejsze kolory jest zgodny z założonym wzorem $\sigma_i \geq \sigma_{i+1} \geq 0$.}
\end{figure}

Twierdzenie związane z SVD zakłada, że jeśli $\mathbf{A}$ jest macierzą o wymiarach $m \times n$ możliwy jest jej rozkład według wartości osobliwych. Aby to wykazać, posłużmy się macierzą $\mathbf{A^\top A}$. Jest to macierz symetryczna, toteż wszystkie jej wartości własne są liczbami rzeczywistymi i istnieje taka macierz ortogonalna $\mathbf{V}$, która ją diagonalizuje. Co więcej, jej wartości własne muszą być nieujemne. Aby to zobaczyć, niech $\lambda$ będzie wartością własną $\mathbf{A^\top A}$ a $\vb{x}$ wektorem własnym związanym z $\lambda$. Wynika z tego, że
\[||\mathbf{A} \vb{x}||^2 = \vb{x}^\top \mathbf{A^\top A}\vb{x} = \lambda\vb{x}^\top \vb{x} = \lambda||\vb{x}||^2 \]

Aby wyznaczyć $\lambda$ z powyższego równania wykonujemy dzielenie
\[\lambda = \frac{||\mathbf{A}\vb{x}||^2}{||\vb{x}||^2} \geq 0 \]

Podniesienie do kwadratu dowodzi, że $\lambda$ jest dodatnia. Załóżmy, że kolumny macierzy $\mathbf{V}$ zostały uporządkowane tak, że odpowiednie wartości własne spełniają zależność
\[\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_n \geq 0 \]

Wartości własne macierzy $\mathbf{A}$ są dane
\[\sigma_i = \sqrt{\lambda_i} \quad i = 1, 2,..., n \]

Niech r oznacza rząd macierzy $\mathbf{A}$. Należy zauważyć, że macierz $\mathbf{A^\top A}$ też będzie rzędu r. Ponieważ $\mathbf{A^\top A}$ jest symetryczna jej rząd będzie równy liczbie niezerowych wartości własnych.
\[\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_r \geq 0 \qquad \lambda_{r+1} = \lambda_{r+2} = \ldots = \lambda_n \geq 0 \]

Ta sama zależność zachodzi dla wartości osobliwych
\[\sigma_1 \geq \sigma_2 \geq \ldots \geq \sigma_r \geq 0 \qquad \sigma_{r+1} = \sigma_{r+2} = \ldots = \sigma_n \geq 0 \]

Dla naszych dalszych rozważań konieczne jest przedstawienie poszczególnych czynników $\mathbf{U}$, $\mathbf{D}$ i $\mathbf{V}$ w rozbiciu na mniejsze macierze, z których są złożone. Zaczynając od macierzy V mamy
\[\mathbf{V}_1 = (\vb{v}_1, \vb{v}_2,... \vb{v}_r), \quad \mathbf{V}_1 = (\vb{v}_{r+1}, \vb{v}_{r+2},... \vb{v}_n) \]

\[
\mathbf{D}_1 = \begin{bmatrix}
\sigma_1 & & & \\
& \sigma_2 & & \\
& & \ddots & \\
& & & \sigma_r
\end{bmatrix}
\qquad
\mathbf{D} = \begin{bmatrix}
\mathbf{D}_1 & 0 \\
0 & 0 \\
\end{bmatrix}
\]
Wektory $\mathbf{V}_2$ stanowią wektory własne macierzy $\mathbf{A^\top A}$ związane z $\lambda = 0$
\[\mathbf{A}^\top \mathbf{A} \vb{x}_i = 0 \quad i = r+1, r+2,..., n \]
\[\mathbf{AV}_2 = 0 \]

Ponieważ V jest macierzą ortogonalną możemy zapisać
\[\mathbf{I = VV^\top} \]
\[\mathbf{A = AI = AVV^\top} \]

Zostało nam jeszcze skonstruować macierz ortogonalną $\mathbf{U}$ o wymiarach $m\times m$ taką, że
\[\mathbf{A = UDV^\top} \]

Co po przeniesieniu $\mathbf{V}$ na lewą stronę możemy równoważnie zapisać jako
\[\mathbf{AV = UD} \]

Porównując pierwsze r kolumn po obu stronach możemy zauważyć
\[\mathbf{A} \vb{v}_i = \sigma_i \vb{u}_i \quad i = 1, 2,..., n \]

Zatem jeśli, ustalimy 
\[\vb{u}_i = \frac{1}{\sigma_i}A \vb{v}_i \quad i = 1, 2,..., n \]

\[\mathbf{U}_1 = (\vb{u}_1, \vb{u}_2,..., \vb{u}_r) \]

To będzie wynikać z tego
\[\mathbf{AV_1 = U_1 D_1} \]

\[\vb{u}_i^\top \vb{u}_j = \left(\frac{1}{\sigma_i}\vb{v}_i^\top \mathbf{A}^\top \right) \left(\frac{1}{\sigma_j}\mathbf{A} \vb{v}_j \right)
= \frac{1}{\sigma_i \sigma_j}\vb{v}_i^\top \left( \mathbf{A^\top A} \vb{v}_j \right)
= \frac{\sigma_j}{\sigma_i} \vb{v}_i^\top \vb{v}_j 
= \delta_ij \]

Jeśli $\vb{u}_1,..., \vb{u}_m$ tworzy bazę ortonormalną dla $\mathbb{R}^m$ to $\mathbf{U}$ jest macierzą ortogonalną. Wówczas ostatnim krokiem jest wykazanie, że $\mathbf{A}$ rzeczywiście jet równe $\mathbf{UDV^\top}$
\[\mathbf{UDV^\top} = \begin{bmatrix}
\mathbf{U_1} & \mathbf{U_2}
\end{bmatrix}
\begin{bmatrix}
\mathbf{D_1} & 0 \\
0 & 0 \\
\end{bmatrix}
\begin{bmatrix}
\mathbf{V_1^\top} \\
\mathbf{V_2^\top} \\
\end{bmatrix}
= \mathbf{U_1 D_1 V_1^\top = AV_1 V_1^\top = A} \]

\subsection*{\Large{Interpretacja geometryczna rozkładu SVD}} \addcontentsline{toc}{subsection}{Interpretacja geometryczna rozkładu SVD}

W interpretacji geometrycznej wartości osobliwe mogą być rozumiane jako długości półosi elipsy na płaszczyźnie, co pokazano na rysunku. Wówczas zakładając, że $\mathbf{A}$ stanowi przekształcenie liniowe przestrzeni - macierze $\mathbf{U}$ i $\mathbf{V}$ reprezentują rotacje i odbicie tej przestrzeni, a $\mathbf{D}$ odpowiada za skalowanie. Innymi słowy, rozkład SVD rozkłada dowolne przekształcenie liniowe na złożenie funkcji trzech przekształceń: obrotu lub odbicia ($\mathbf{V}$), skalowania ($\mathbf{D}$) i kolejnego obrotu lub odbicia ($\mathbf{U}$). W szczególności, jeśli wyznacznik poddawanej przekształcaniu macierzy jest dodatni, wówczas macierze $\mathbf{U}$ i $\mathbf{V}$ mogą odpowiadać zarówno za rotację jak i odbicie. Jeśli wyznacznik jest ujemny tylko jedna z nich odpowiada za odbicie, jeśli jest równy zero - odbicia nie ma wcale.

\begin{figure}[h]
\centering
\includegraphics[width=10cm]{svd_geom}
\caption{Interpretacja geometryczna SVD macierzy $\mathbf{A}$ o wymiarach 2$\times$2. Celem jest przekształcenie przestrzeni pokazanej w lewym górnym rogu, tak aby uzyskać obraz widoczny w prawym górnym. Transformacja zostaje rozłożona przez SVD na trzy etapy: rotację, skalowanie i ponowną rotację.}
\end{figure}

Taka interpretacja może również zostać uogólniona do $n$-wymiarowych przestrzeni euklidesowych. Wówczas wartości osobliwe dowolnej macierzy kwadratowej n $\times$ n staną się długościami półosi $n$-wymiarowej elipsoidy. Wartości osobliwe zawierają w sobie informację o długościach a wektory osobliwe - o kierunku półosi. Nie jest to jednak zbyt praktyczne rozwiązanie, dlatego mówiąc o rozkładzie SVD w proponowanym algorytmie będziemy posługiwać się uproszczonym SVD. Przede wszystkim, zakładamy, że dysponujemy szablonem, który ma dokładnie te same wymiary co szukany obiekt oraz szukany przedmiot jest bryłą sztywną (więc również jego szablon traktujemy w ten sam sposób). Te założenia sporo upraszczają. Przede wszystkim, odnosząc się do praktycznego zastosowania możemy pominąć operację skalowania.

\newpage
\subsection*{\LARGE{4. Algorytm Iterative Closest Point}} \addcontentsline{toc}{section}{4. Algorytm Iterative Closest Point}

Mając teoretyczne podstawy za sobą możemy przejść do implementacji końcowego programu stworzonego w oparciu o algorytm ICP (ang. \emph{Iterative Closest Point}). Dla danej chmury punktów jego celem jest znalezienie położenia i orientacji obiektu, który przedstawia. W tej sytuacji dane wejściowe stanowią: chmura punktów i szablon szukanego obiektu, do którego chcemy dopasować widoczny przedmiot. Przez dopasowanie rozumiemy znalezienie transformacji (obrotu i przesunięcia) wykrytego obiektu względem początkowego położenia szablonu. Możemy rozpisać pojedynczą iterację algorytmu w postaci listy kroków:

\begin{enumerate}
\item Wyznaczenie centroidów $\vb{a}$ i $\vb{b}$ dla obu chmur punktów $\mathbf{A}$ i $\mathbf{B}$.
\item Utworzenie $\mathbf{A'}$ i $\mathbf{B'}$ przez standaryzację wartości w $\mathbf{A}$ i $\mathbf{B}$. Standaryzacja odbywa się przez odjęcie od każdego punktu centroidu z całej chmury.
\item Obliczenie macierzy $\mathbf{H}$ jako iloczynu $\mathbf{A'}$ i $\mathbf{B'}$.
\item Rozkład $\mathbf{H}$ według wartości osobliwych.
\item Wyznaczenie macierzy obrotu i jej wyznacznika w celu sprawdzenia czy nie zaszedł przypadek odbicia.
\item Obrócenie centroidu $\vb{a}$ zgodnie z macierzą rotacji i obliczenie wektora przesunięcia jako różnicy centroidów. 
\end{enumerate}

Dane są dwa zbiory punktów: $\mathbf{A}$ i $\mathbf{B}$ w postaci macierzy o wymiarach 3$\times$n. Każdy wiersz w obu z nich opisuje pojedynczy punkt w postaci współrzędnych x, y, z. Równanie opisujące przejście punktu z jednego zbioru do drugiego wygląda następująco
\[\mathbf{A = R B + T + N} \]

Gdzie $\mathbf{R}$ jest macierzą rotacji o wymiarach 3 $\times$ 3, $\mathbf{T}$ - wektorem przesunięcia o wymiarach 3 $\times$ 1 a $\mathbf{N}$ - wektorem szumu. Zakładamy też, że obrót odbywa się wokół początku układu. Chcemy dobrać takie $\mathbf{R}$ i $\mathbf{T}$, aby zminimalizować wyrażenie
\[d^2(\vb{a}, \vb{b}) = \sum_{i=1}^n \| \vb{b}_{i} - (\mathbf{R} \vb{a}_i + \mathbf{T})\|^2 \]

Niech 
\[\vb{a} = \frac{1}{n} \sum_{i=1}^n \vb{a}_i \qquad \vb{b} = \frac{1}{n} \sum_{i=1}^n \vb{b}_i \]

Wówczas $\vb{a'}$ i $\vb{b'}$ będą zbiorami odległości poszczególnych punktów w zbiorach $\mathbf{A}$ i $\mathbf{B}$ od ich centroidów
\[\vb{a'_i = a_i - a \qquad b'_i = b_i - b} \]

Mamy
\[d^2(\vb{a'}, \vb{b'}) = \sum_{i=1}^n \| \vb{b}_{i} - \mathbf{R} \vb{q}_{i}\|^2 \]

Następnym krokiem jest wyznaczenie macierzy 3 $\times$ 3 
\[\mathbf{H} = \sum_{i=1}^n \vb{a}_i^{'} \vb{b}_{i}^{'\top} \]

I rozłożenie jej na wartości osobliwe
\[\mathbf{H = UDV^\top} \]

Ponieważ, zgodnie z tym co zostało wspomniane podczas interpretacji geometrycznej rozkładu SVD, macierz $\mathbf{D}$ jest odpowiedzialna za skalowanie - podczas wyznaczenia wynikowej rotacji możemy ją odrzucić. W ten sposób otrzymujemy wyrażenie
\[\mathbf{R = VU^\top} \]

Jeśli wyznacznik $\det \mathbf{R} = 1$ to  $\mathbf{R}$ stanowi macierz obrotu. W przeciwnym wypadku, gdy mamy do czynienia z wyznacznikiem $\det \mathbf{R}= -1$ algorytm nie zwraca rozwiązań, ale ten przypadek z reguły nie występuje. Gdy obrócimy centroid zbioru $\mathbf{A}$ zgodnie z wyznaczoną właśnie macierzą rotacji to wektor przesunięcia $\vb{t}$ będzie stanowił różnicę między dwoma centroidami.
\[\vb{t} = \vb{b}^\top - \mathbf{R} \vb{a}^\top \]

Wynikową transformację możemy osiągnąć w odpowiedni sposób konkatenując macierz obrotu z wektorem przesunięcia
\[\mathbf{T} = \left[
\begin{array}{ccc|c}
& & & \\
& \mathbf{R} & & \vb{t}^\top \\
& & & \\
\hline
0 & 0 & 0 & 1
\end{array}
\right]
\]

Obliczona transformacja nie stanowi jeszcze ostatecznego rozwiązania algorytmu. Możemy jedynie stwierdzić, że poddany tej transformacji obiekt znajduje bliżej docelowego szablonu niż był do tej pory. Dążąc do ostatecznego wyniku, potrzebujemy ponownie wykonywać poprzednie kroki do momentu, aż odległości między korespondującymi punktami w zbiorach staną się akceptowalnie małe. Każdy taki etap nosi nazwę \textbf{iteracji}, dlatego też algorytm nazywany jest iteracyjnym.

\begin{figure}[h]
\centering
\includegraphics[width=10cm]{mean_dist}
\caption{Wykres średnich odległości korespondujących punktów między dwiema chmurami przeprowadzony dla rotacji w zakresie 0.25$\pi$. Zależność ta została powtórzona przez największą liczbę prób. Z przebiegu widać, że algorytm w największej liczbie przypadków znajdował ostateczną transformację przy osiemnastej iteracji.}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{bunny_icp}
\caption{Działanie algorytmu ICP na przykładzie obiektu w kształcie królika.}
\end{figure}

\newpage
\subsection*{\LARGE{Implementacja algorytmu}} \addcontentsline{toc}{subsection}{Implementacja algorytmu}
Na następnych stronach zamieszczona została minimalistyczna implementacja omówionej listy kroków w języku Python. Pełna wersja algorytmu ICP wraz z komentarzami znajduje się w sekcji \emph{Appendix}.

\newpage
\lstinputlisting[language=Python]{transform_matrix.py}
\vspace{5mm}

Funkcja \emph{transform matrix} przyjmuje dwa argumenty w postaci macierzy \textbf{A} i \textbf{B} i. \textbf{A} jest macierzą obiektu, który chcemy dopasować do szablonu \textbf{B}. W rozważanym przypadku obie macierze są wymiaru 3 $\times$ n, czyli składają się z trzech kolumn, z których pierwsza zawiera wartości współrzędnej x, druga - y, a trzecia - z. Każdy wiersz w obu macierzach reprezentuje jeden z $n$ punktów. W linijkach trzeciej i czwartej następuje wyznaczenie centroidów \textbf{A} i \textbf{B} w postaci wektorów o długości trzy przez obliczenie średniej dla każdej kolumny współrzędnych. Macierze \textbf{A1} i \textbf{B1} zostają utworzone przez odjęcię od \textbf{A} i \textbf{B} ich centroidów. Pomaga nam to ustandaryzować wartości zmiennych. W linijce dziewiątej liczony jest iloczyn nowo utworzonych macierzy (transponujemy \textbf{A1}, żeby miała tyle samo kolumn co B1 wierszy). Następnie następuje rozkład \textbf{H} według wartości osobliwych, który zwraca trzy czynniki w postaci macierzy \textbf{U}, \textbf{D} i \textbf{Vt}. Mnożąc przez siebie \textbf{U} i \textbf{Vt} (obie transponowane) otrzymujemy otrzymujemy macierz rotacji R. Warunek poniżej obsługuje specjalny przypadek odbicia. Wektor przesunięcia \textbf{t} jest liczony jako różnica centroidu \textbf{B} i obróconego przez \textbf{R} centroidu \textbf{A}. Funkcja zwraca przekształcenie jako \textbf{R} i \textbf{t}.

Gdy mamy zaimplementowane wyznaczanie transformacji dla pojedynczej iteracji możemy przejść do ostatecznego algorytmu ICP.

\vspace{5mm}
\lstinputlisting[language=Python]{icp.py}
\vspace{5mm}

Funkcja \emph{icp} przyjmuje cztery argumenty w tym jeden domyślny. Przyjmowane są zmienne \textbf{A} i \textbf{B} jako macierze o wymiarach 3 $\times$ n, maksymalna liczba iteracji oraz domyślnie ustawiony próg tolerancji. W linijkach 4-7 tworzone są macierze \textbf{A1} i \textbf{B1} jako czterokolumnowe odpowiedniki \textbf{A} i \textbf{B}. Dzieje się to przez skopiowanie wartości \textbf{A} i \textbf{B} i dostawienie kolumny jedynek. Linijki 10-19 zawierają główną pętlę programu wykonującą się maksymalnie \emph{max iterations} razy. Przed wejściem do pętli została zainicjalizowana zmienna \emph{prev error}, która będzie nadpisywana przy każdej iteracji. Następnie funkcja \emph{calculate distance} zwraca odległości euklidesowe między poszczególnymi punktami i ich indeksy. Będą one potem nam potrzebne do wyznaczenia średniego błędu dopasowania. Wyznaczana jest najlepsza transformacja \textbf{T} między \textbf{A1} i \textbf{B1}, po czym zostaje ona zastosowana do przesunięcia \textbf{A1}. Pod koniec działania pętli liczony jest średni błąd jako średnia odległości między punktami. Jeśli wartość bezwzględna z różnicy średniego i poprzedniego błędu jest większa niż przyjęta toleracja, a dotychczasowa liczba iteracji mniejsza niż \emph{max iterations} - pętla wykonuje się ponownie. W momencie znalezienia wystarczająco dokładnej transformacji przed zakończeniem pętli - jest ona przerywana.

\subsection*{\LARGE{Działanie robota w praktyce}} \addcontentsline{toc}{subsection}{Działanie robota w praktyce}

Przejście od teorii do praktyki wymagało dodatkowo implementacji funkcji odpowiedzialnych za poruszanie manipulatorem. Został w tym celu użyty popularny middleware ROS (\emph{Robot Operating System}), do którego odwołania były kierowanie z kodu Pythona. Omawiając program, możemy zwrócić uwagę, że wykorzystane zostały dwa serwisy: jeden odpowiedzialny za chwytak i drugi - za przeguby manipulatora. Funkcja \emph{move gripper} sterująca chwytakiem przyjmuje argument logiczny otwierający go w przypadku prawdy i zamykający w przypadku przeciwnym. Metoda \emph{move} kontrolująca ustawienie manipulatora jako argument przyjmuje z kolei tablicę sześciu wartości opisujących dane miejsce w przestrzeni przez jego współrzędne kartezjańskie i kąty Eulera.

\vspace{5mm}
\lstinputlisting[language=Python]{gripper.py}

\newpage
\lstinputlisting[language=Python]{move.py} 
\vspace{5mm}

Również z ROSa korzystając zostało obsłużone zachowanie kamery. W tej roli została użyta niewielkich rozmiarów kamera \emph{Intel RealSense} zdolna zapewnić obraz w formacie RGBD. W linijce 26. przytoczonego kodu źródłowego znajduje się funkcja odpowiedzialna za zrobienie zdjęcia. Zapisuje ona bieżącą klatkę do pliku, który następnie może zostać załadowany do programu korzystającego z ICP w celu wyznaczenia ostatecznej transformacji.

\vspace{5mm}
\lstinputlisting[language=Python]{camera.py}
\vspace{5mm}

Mimo wszelkich starań działanie proponowanego systemu w praktyce często różniło się od pożądanego. Głównym problemem była rozbieżność między chmurą punktów widzianą z kamery a rzeczywistym kształtem przedmiotu. Gdy ta rozbieżność była zbyt duża - algorytm nie był w stanie odnaleźć poprawnego przekształcenia. Przez to w niektórych przypadkach konieczne okazało się znajdowanie pozycji obiektu zastępując niekształtną chmurę punktów jej centroidem. Jest to metoda działająca w przypadku odpowiednio małych obiektów, które trudno wykryć za pomocą kamery, natomiast nie dostarcza tak dokładnych informacji jak proponowane rozwiązanie.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{robot}
\caption{Doświadczenie wykonane przez ramię robota UR5. Drewniany klocek niewielkich rozmiarów z trudem został wyodrębniony na tle chmury punktów pobranej z kamery, natomiast ze wzgledu na maksymalny rozstaw szczęk chytaka niemożliwe było chwytanie większych elementów.}
\end{figure}

\newpage
\section*{\LARGE{5. Podsumowanie i wnioski}} \addcontentsline{toc}{section}{5. Podsumowanie i wnioski}

W tej pracy została zaproponowana metoda estymacji położenia obiektu, którego modelem dysponujemy. Rozważony problem jest kluczowy na polu współczesnej robotyki, głównie przez trend do stosowania coraz bardziej "elastycznych" sposobów sterowania manipulatorami. Problem został rozbity na podstawowe zagadnienia, których rozwiązania również zostały przeprowadzone. Ponieważ jest nie jest to temat nowy, podczas pisania pracy było wykorzystanych wiele obecnie istniejących źródeł opisujących pełne lub cząstkowe rozwiązanie problemu. Tak czy inaczej, główną motywacją było znalezienie możliwie prostego i skutecznego rozwiązania, co zostało osiągnięte. 

Mając na uwadze dotychczasowe rozwiązania opisane w rozdziale drugim, pomysł na opracowanie kolejnej metody interakcji maszyn z otoczeniem może się wydawać co najmniej błahy, a w najgorszym wypadku zbędny. Takie stwierdzenia jednak tracą na sile po bliższym przyjrzeniu się motywacjom jakie przyświecały powstaniu tej pracy. Gdybyśmy chcieli wykorzystać sieci neuronowe lub uczenie przez wzmacnianie opisane w rozdziale 2. wiązałoby się to z dodatkowym czasem jaki trzeba by było poświęcić na przetrenowanie tych modeli. Metody widzenia maszynowego z kolei nie są najczęściej przystosowane do radzenia sobie z analizą przestrzeni trójwymiarowych. Poza tym, te algorytmy wydają się nadmiernie złożone, co często utrudnia pracę z nimi. Trudno jest natomiast znaleźć wymierny wskaźnik poziomu skomplikowania danego algorytmu. Moglibyśmy posłużyć się miarą złożoności Kołmogorowa, która określa jak długi musi być najkrótszy możliwy kod, aby wygenerować dany obiekt, w tym wypadku macierz transformacji.  Niestety, ponieważ w składni programów znajduje się wiele odwołań i wcześniej zaimplementowanych funkcjonalności - złożoność Kołmogorowa jest w stanie jedynie zgrubnie oszacować poziom komplikacji, dlatego nie została przeze mnie zastosowana.

Z drugiej strony, istnieje wiele podobieństw między proponowaną pracą a przedstawionymi metodami uczenia maszynowego. Oba podejścia rozwiązują problem iteracyjnie próbując z każdym krokiem zmniejszyć wartość średniego błędu. W przypadku sieci neuronowych mówimy o funkcji straty, \emph{Reinforcement Learning} posługuje się terminologią nagrody i kary, w proponowanym pomyśle opartym o SVD mamy miarę dopasowania. W pożądanych przypadkach, miary te najczęściej maleją wraz z czasem o coraz mniejsze wartości aż funkcja stanie się niemalejąca. Można wtedy powiedzieć, że dany algorytm poprawnie wykonał swoje zadanie.

Podsumowując, praca, mimo wszelkich zalet nie stanowi samowystarczalnego systemu, który mógłby w obecnym stanie zostać wykorzystany na potrzeby przemysłu. Przede wszystkim dlatego, że obejmuje tylko część zagadnień jakie należałoby rozwiązać przed procesem komercjalizacji. Czynności takie jak nadzór działania czy zorientowanie chwytaka względem szablonu obiektu nadal wymagają interwencji człowieka. Projekt ma za to potencjał do stania się częścią większego oprogramowania, gdzie stanowiłby znaczny wkład na polu manipulacji i przemysłu 4.0.

\subsection*{\LARGE{Dalszy rozwój}} \addcontentsline{toc}{subsection}{Dalszy rozwój}

Mimo, że zaproponowana metoda estymacji położenia obiektu wykonała założone zadanie z powodzeniem, zastosowana technika posiada również pewne ograniczenia. Do najważniejszych z nich należy brak możliwości wyłonienia pożądanego przedmiotu ze zbioru zawierającego więcej niż jeden element. Tą kwestię można oczywiście rozwiązać stosując metody segmentacji. Mielibyśmy wtedy chmurę punktów rozbitą na kilka elementów, ale rodziłoby to kolejny problem - algorytm musiałby rozpoznać, który z nich stanowi żądany obiekt. W tym miejscu mogłaby zostać zaimplementowana logika odpowiadająca za obsługę przypadków w zależności czy w danych, które są sprawdzane mamy do czynienia z jednym, kilkoma lub żadnym szukanym obiektem. Podsumowując, dalszy rozwój tej pracy mógłby obejmować:

\begin{itemize}
\item \textbf{Odszumienie} wejściowej chmury punktów przez odcięcie powierzchni stołu i pozbycie się nieistotnych elementów podczas przetwarzania danych.

\item \textbf{Segmentację} w celu sprawdzenia z iloma elementami mamy do czynienia i gdzie się one znajdują.

\item \textbf{Rozpoznanie} jak bardzo dany przedmiot odbiega od swojego szablonu. To pozwoliłoby stwierdzić czy mamy do czynienia z właściwym obiektem. Jeśli tak nie jest - dalsza estymacja położenia nie ma sensu.
\end{itemize}

Odszumienie przychodzących danych w przypadku chmur punktów nie jest łatwym zajęciem. Może zostać przeprowadzone ręcznie przez wyśrodkowanie obiektu i "przycięcie" chmury wzdłuż jego granic. W przypadku gdy oś z jest skierowana do góry i prostopadła do stołu, punkty do niego należące mogą zostać odcięte przez usunięcie wszystkich punktów, dla których z jest większe od wysokości stołu. Wymaga to jednak pewnej interwencji człowieka. Chcąc ten proces zautomatyzować moglibyśmy posłużyć się metodą RANSAC (\emph{Random sample consensus}) - algorytmem iteracyjnym do usuwania z danych elementów odstających. Problem ten można uprościć również zaniżając pozycję kamery względem stołu, dzięki czemu stosunek liczby punktów obiektu do wszystkich punktów byłby większy. 

Wracając do tematu segmentacji, zadanie to dla niewielkiej liczby elementów może zostać zrealizowane przy pomocy technik analizy skupień, do którego zaliczamy algorytmy uczenia nienadzorowanego takie jak DBSCAN, grupowanie hierarchiczne i metodę $k$-średnich. Jeśli podział zostałby zrealizowany dla każdej liczby klastrów od 1 do $n$ (przy czym $n$ to założona maksymalna liczba klastrów) - wówczas do dyspozycji mielibyśmy miarę dopasowania do każdej z tych ewentualności. Taką miarą jest najczęściej kryterium sumy kwadratów w obrębie klastra WCSS \emph{(Within-Cluster Sum of Squared)}. Z niej możliwe stałoby się odczytanie optymalnej liczby elementów. 

Niestety, taka metoda jest zachłanna obliczeniowo, przez co zużywa sporo czasu i może być stosowana tylko do małych $n$. Wówczas, liczba klastrów rozważanych w każdej iteracji rośnie o jeden zgodnie z ciągiem arytmetycznym: 1, 2, 3, ... $n$. Ponieważ, w każdej iteracji jest mowa o innych klastrach ich sumaryczną ilość przedstawia ciąg: 1, 3, 6, 10, 15, ... $\sum n$. Innym powodem, który utrudnia pracę w wieloma elementami jest przede wszystkim to, że im większy jest ten zbiór - tym więcej charakterystycznych cech obiektów może być przysłonięte przez inne elementy. Trudno jest się z nim uporać w sposób obliczeniowy, ale może zostać w najprostszy sposób rozwiązany przez umieszczenie elementów na podajniku wibracyjnym.

Z drugiej strony do rozwiązania problemu wielu obiektów mogłyby zostać zaprzęgnięte sieci neuronowe, a szczególną uwagę w tym względzie przykuwają modele tj. \emph{Mask R-CNN} i \emph{PointNet}. Pierwsza z nich jest przykładem sieci R-CNN (ang. \emph{Region Based Convolutional Neural Networks}) przygotowanym specjalnie pod problem segmentacji. Natomiast struktura sieci \emph{PointNet} została przystosowana do radzenia sobie z danymi w postaci chmur punktów i była testowana w zadaniach klasyfikacji i segmentacji. Niestety, jak większość sieci neuronowych, obie są przykładem uczenia nadzorowanego, a więc ich użycie wymaga etykietowania danych, na których będą trenowane. Jeśli zbiór uczący zostanie utworzony w postaci obrazów RGBD, a każdy kształt przedmiotu oznaczymy innym kolorem, wówczas takie etykietowanie może być przeprowadzone korzystając z nałożenia na siebie informacji o kolorze (RGB) i głębokości (D). Mówiąc prościej, mamy "zlepek" różnych kształtów, który chcemy podzielić na klastry. Kompletna informacja o nich, tj. ich liczba i położenie jest już zawarta w obrazie, więc tym co możemy zrobić jest przenieść te dane przypisując do każdego widzianego punktu w przestrzeni jego klaster reprezentowany przez kolor przedmiotu, do którego należy. To oczywiście niesie za sobą komplikacje w postaci różnego oświetlenia i cieni, ale na ogół nie stanowią one większego problemu.

\newpage
\section*{\LARGE{Bibliografia}} \addcontentsline{toc}{section}{Bibliografia}
\begin{enumerate}
\item \emph{Wprowadzenie do robotyki} - John. J. Craig
\item \emph{Legged Robots That Balance} - Marc H. Raibert
\item \emph{The Senses Considered as Perceptual Systems} - James J. Ginson
\item \emph{Biocybernetyka} - Ryszard Tadeusiewicz
\item \emph{Odkrywanie właściwości sieci neuronowych} - Ryszard Tadeusiewicz
\item \emph{The Representation of Tool Use in Humans and Monkeys: Common and Uniquely Human Features}, "Journal of Neuroscience" - Scott H. Jonhson-Frey
\item \emph{Krótka historia rozumu. Od pierwszej myśli do rozumienia wszechświata} - Leonard Mlodinow
\item \emph{Deep Learning. Systemy uczące się} - Ian Goodfellow, Yoshua Bengio, Aaron Courville
\item \emph{Solving Rubik's Cube with a Robot Hand} - praca zbiorowa OpenAI
\item \emph{Linear Algebra with Applications} - Steven J. Leon
\item \emph{A Singularly Valuable Decomposition} - Dan Kalman
\item \emph{A Method for Registration of 3-D Shapes} - Paul J. Besl, Neil D. McKay
\item \emph{Least-Squares Fitting of Two 3-D Point Sets} - K.S. Arun, T.S. Huang, S.D. Blostein
\item \emph{Mask R-CNN} - Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick
\item \emph{PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation} - Charles R. Qi, Hao Su, Kaichun Mo, Leonidas J. Guibas
\end{enumerate}

\newpage
\section*{\LARGE{Appendix}}
\subsection*{\Large{Kod programu}}
\lstinputlisting[language=Python]{appendix.py}

\end{document}