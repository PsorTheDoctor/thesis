\documentclass[12pt]{article}
\usepackage{polski}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amsfonts}
\graphicspath{{./images/}}

\begin{document}
\title{Rozpoznawanie i estymacja pozycji obiektu z użyciem systemu wizyjnego w zadaniu chwytania realizowanego przez manipulator}
\author{
	Praca inżynierska \\\\
	Adam Wołkowycki \\\\
	Promotor: Dr Adam Wolniakowski
}
\maketitle

\newpage
\section{Abstract}
In the recent years, a rising trend in application of autonomous robots can be observed. Nowadays, many solutions intended to this part of market are developed and each of these is based on solutions that are hard to algorithmizable. The examples of this kind of tasks are image processing, object position estimation and configuration of kinematics. I am going to focus on them in my thesis.

The purpose of this thesis is to develop a solution resposible for recognition a given object and perform grasping it by a manipulator. The further development of the problem known as \emph{bin picking} aims at increasing a scope of industrial robots' operation in a context of their work.

Because the tasks I mentioned are often performed by a particular type of robots, in my work I am going to use UR5 manipulator by Danish manufacturer Universal Robots. There will be implemented algorithm on it that will be able to define a location of seen objects based on the data from the computer vision sensors. In my work I will be using code examples written in Python. Thanks to this, it will be possible to deeply verified in practice the working of a robot.

\newpage
\section{Streszczenie}
W ostatnich latach można zaobserwować wzrastający trend na zastosowanie robotów o działaniu autonomicznym. Powstaje obecnie wiele rozwiązań przeznaczonych dla tego segmentu rynku, a każdy z nich opiera swoje działanie o rozwiązania, które są przyjęte jako trudno algorytmizowalne. Przykładami takich zadań są przetwarzanie obrazu, estymacja położenia obiektów i konfiguracja kinematyki robota. Właśnie na nich zamieram skupić się w mojej pracy.

Celem pracy jest opracowanie rozwiązania odpowiedzialnego za rozpoznawanie wskazanego obiektu i realizację chwycenia go przy wykorzystaniu manipulatora. Rozwinięcie problemu znanego pod nazwą \emph{bin picking} ma przede wszystkim na celu zwiększenie zakresu działania robotów przemysłowych w kontekście ich pracy.

Ponieważ zadania, o których wspomniałem są często realizowane przez określony rodzaj robotów, w swojej pracy zamierzam wykorzytać manipulator UR5 duńskiego producenta Universal Robots. Na nim zostanie zaimplementowany algorytm, który w oparciu o dane z systemu wizyjnego będzie w stanie określić położenie widzianych przedmiotów. W pracy posłużę się programami stworzonymi w języku Python. Dzięki temu działanie robota będzie można gruntownie zweryfikować w praktyce.

\newpage
\section{Wprowadzenie}
Definiując współczesną robotykę jako dziedzinę wiedzy mamy na uwadze przede wszystkim dwa problemy: natury konstrukcyjnej i sterowania. Ponieważ wielu producentów z branży robotyki oferuje szeroką gamę rozwiązań spełniających potrzeby niniejszej pracy, posłużymy się robotem industrialnym, w którego konstrukcję nie będziemy ingerować. Dlatego, aby zrozumieć koncepcje zawarte w tej pracy nie jest wymagana dogłębna znajomość budowy robota. Natomiast, jeśli chodzi o sterowanie to możemy ponownie wyodrębnić dwie podrzędne kategorie: manipulację i lokomocję. 

\begin{itemize}
\item \textbf{Manipulację} definiujemy jako zdolność do wykonywania precyzyjnych ruchów przez efektor końcowy robota w celu osiągnięcia wyznaczonego celu, np. przeniesienia przedmiotu procesowanego - tzw. zadanie \emph{pick and place}. Roboty posiadające tą cechę nazywamy mianem manipulatorów.

\item \textbf{Lokomocja} z kolei, jak sama nazwa wskazuje, opisuje zdolność do przemieszczania się platformy mobilnej. Jest kluczowa m.in. dla pojazdów AVG \emph{(Automated Guided Vehicles)}, łazików czy robotów kroczących. Ponieważ w swojej pracy zamierzam skupić się na robotach stacjonarnych, nie będę wracał więcej do tego tematu.
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=10cm]{classification}
\caption{Podział robotyki na odrębne zagadnienia.}
\end{figure}

Musimy jednak mieć na uwadze, że jest to dość ogólna i luźna klasyfikacja. Pierwotnie, podobny podział zaproponował Marc H. Raibert kończąc książkę  \emph{Legged Robots That Balance} w ostatnim rozdziale w pytaniu \emph{Do Locomotion and Manipulation Have a Common Ground?} Dalej problem został rozwinięty w kontekście badań nad lokomocją i manipulacją oraz tego jak te dwie dziedziny wpływają na siebie nawzajem.

\subsection{Manipulacja a percepcja}
Gdy mamy już wyjaśnione ogólne zagadnienia możemy skupić się na danym zadaniu manipulatora. Załóżmy, że zadaniem robota jest pobranie kilku kostek z podajnika w miejscu A i przeniesienie ich w odpowiednie miejsca na palecie B, tzw. paletyzacja. Aby wykonać to zadanie możemy podejść do niego na dwa sposoby: zapewniając maszynie odpowiednią percepcję lub nie. 

\begin{itemize}
\item \textbf{Przypadek 1. Brak percepcji} \\
Brak percepcji oznacza brak danych wejściowych. Operujemy jedynie na wielkościach, które muszą zostać założone z góry. W tym przykładzie będą to: pozycje bazy i kiści manipulatora, podajnika, palety oraz rozmiar kostki. Z tego powodu, robot pozbawiony percepcji zawsze musi otrzymać od nas dokładne informacje odnośnie trajektorii ruchu.

\item \textbf{Przypadek 2. Percepcja z użyciem systemu wizyjnego} \\
Przez system wizyjny możemy rozumieć dowolny rodzaj kamery, jak również projektory chmur punktów i urządzenia nakładające na siebie kolory z danymi o głębokości obrazu, zwane kamerami RGBD. W takim przypadku operujemy znaczną ilością danych, dzięki czemu znajomość wymienionych wyżej wartości nie musi być konieczna - możemy oszacować te wartości używając metod widzenia maszynowego \emph{(Computer Vision).} Zastosowanie systemu wizyjnego możemy ponownie podzielić na dwa typy: z kamerą zamontowaną nieruchomo względem bazy robota oraz na jego narzędziu. Różnica między nimi polega głównie na tym, że w pierwszym przypadku widziane obiekty procesowane, jeśli robot ich bezpośrednio nie dotyka, pozostają w bezruchu względem kamery, a przez to nie zmienia się ich obraz. Przypadek z kamerą zamontowaną na kiści jest w stanie zapewnić więcej użytecznych ujęć, ale przez to jest też trudniejszy do zaimplementowania.
\end{itemize}

\newpage
\begin{figure}[h]
\includegraphics[width=\textwidth]{basic}
\caption{Przykład zadania paletyzacji jako manipulacji przy braku percepcji. Po lewej widoczny jest kod programu stworzony w języku MELFA-BASIC IV. Po prawej na górze - ujęcie wykonane podczas pracy robota Kawasaki, na dole - widok palety do ukończeniu zadania. Symulacja została wykonana w środowisku Cosimir.}
\end{figure}

\begin{figure}[h]
\includegraphics[width=\textwidth]{bullet}
\caption{Przykład zadania \emph{bin picking} wykonywanego przez robota KUKA w oparciu o dane z systemu wizyjnego. Widoczne po lewej obrazy zostały wygenerowane sztucznie jako hipotetyczne dane z kamery umieszczonej nieruchomo względem bazy robota. Od góry - ujęcie powstałe jako bezpośrednia projekcja, obraz niosący dane o głębii i obraz poddany segmentacji. Symulacja została stworzona w języku Python z wykorzystaniem biblioteki \emph{pybullet}.}
\end{figure}

\subsection{Cele manipulacji}
Chociaż pierwsze w pełni sprawne manipulatory istnieją w sektorze industrialnym od dekad, wykorzystanie ich w zadaniu manipulacji stanowi wyzwanie, podczas gdy często stosowane rozwiązania nie wykorzystują w pełni ich potencjału. Dlatego chcąc określić potrzebę implementacji omawianych tu rozwiązań warto jest przyjrzeć się celom przed, którymi staje współczesna robotyka. Przede wszystkim chcemy, aby dane rozwiązanie było możliwie jak najbardziej \textbf{uniwersalne}, to znaczy działało poprawnie w różnych środowiskach, było odporne na zaszumienia i efektywne. Aby te cechy mogły zostać spełnione należy pewne kwestie muszą zostać rozwiązane, a należą do nich:

\begin{itemize}
\item \textbf{Zdolność do operowania wieloma różnymi obiektami.} Uniwersalność może zostać osiągnięta przez adaptację, czyli trwający w czasie proces, którego celem jest przystosowanie danego podmiotu do obserwowanego środowiska. Algorytmy działające zgodnie z tą zasadą stają się dość popularne szczególnie w ostatnich latach, a techniki wiodące prym na tym polu określane są mianem \emph{Reinforcement Learning} (w języku polskim przyjęły się nazwy uczenia ze wspomaganiem i uczenia z krytykiem). Nie zawsze stanowią jednak wydajne rozwiązanie zadania, głownie ze względu na wspomniany wyżej czas potrzebny do przetrenowania algorytmu. Problematyczne mogą być również dość chaotyczne, a przez to nieoptymalne i trudne do przewidzenia trajektorie jakie algorytmy tego typu generują.

\item \textbf{Sterowanie we wszystkich stopniach swobody}. Manipulator przemysłowy wykorzystany w tej pracy charakteryzuje się sześcioma stopniami swobody (bez chwytaka), co sprawia, że dla niektórych pozycji zadanie kinematyki odwrotnej będzie miało więcej niż jedno rozwiązanie i wiele możliwych trajektorii, z których nie wszystkie są optymalne.

\item \textbf{Odporność na zaszumione lub zniekształcone dane wejściowe}. Ograniczona rozdzielczość kamer RGBD, zaszumienie spowodowane światłem zewnętrznym i różne możliwe rozrzucenie obiektów komplikuje działanie algorytmu i aby proces przebiegał płynnie - musi zostać obsłużone.

Do powyższej listy moglibyśmy także dopisać zdolność do przenoszenia zachowań z symulacji do rzeczywistego świata, ale ponieważ nie we wszytkich problemach stosuje się metody symulacji - nie dopisywałem ich do tej listy.
\end{itemize}

\subsection{Afordancje}
Zdolność oddziaływania na jakiś obiekt lub środowisko została określona mianem \textbf{afordancji} przez psychologa Jamesa J. Gibsona w 1966 roku w książce \emph{The Senses Considered as Perceptual Systems} i opisana w artykule \emph{The  Theory  of  Affordance.  In:  Perceiving,  Acting  and  Knowing  Toward  an Ecological Psychology}. Definicja ta przyjęła szerokie znaczenie w dziedzinach tj. psychologia, kognitywistyka, sztuczna inteligencja czy robotyka.

Możemy zdefiniować afordancje na przykładzie ludzi, ale człowiek ze względu na swoją złożoność często pozostaje niewdzięcznym modelem. Dlatego dla naszych rozważań posłużymy się morskim bezkręgowcem - krabem \emph{Limulus polyphemus}. Przykład ten został opisany m.in. w \emph{Biocybernetyce} Ryszarda Tadeusiewicza. Korzystnymi cechami tego stawonoga z naszego punktu widzenia są posiadanie ośrodkowego układu nerwowego oraz oczu, z których impulsy jesteśmy w stanie śledzić. Tak więc, jesteśmy w stanie w przybliżeniu zobrazować jego pole widzenia. 

Co wobec tego widzi krab? Przede wszystkim \textbf{obiekty} stanowiące najczęściej zagrożenie lub będące przedmiotem łowów niezależnie od tła. W warunkach laboratoryjnych możemy także osiągnąć pobudzenie neuronalne za pomocą punktowego oświetlenia lub gwałtownych zmian światła. Krab nie widzi natomiast równomiernych gradientów oświetlenia czy monotonnego pastelowego tła. Jest to biologicznie uzasadnione, ponieważ informacje te nie są dla niego w żaden sposób niezbędne do życia. 

Przykład kraba, mimo że może wydawać się surowy, niesie ze sobą istotne wskazówki na temat tego co powinniśmy rozumieć przez afordancje i jaka jest ich rola. Przechodząc do chwytania, które w środowisku naturalnym jest domeną naczelnych, aby mogło ono zostać zrealizowane -  wykorzystywane są informacje zawarte w obrazie. Pozwala to tłumaczyć dlaczego niektóre gatunki, przykładem tu mogą być wczesne homidy, wykształciły bardziej rozwinięty wzrok konieczny do wytypowania elementów otoczenia. Podobnie w robotyce, robot musi nauczyć się jak chwytać i operować uchwyconymi obiektami, tak aby osiągnąć wyznaczony cel. Różne przedmioty, np. młotek mogą być uchwycone na wiele różnych sposobów natomiast liczba optymalnych chwytów jest ograniczona w kontekście danego zadania. 

\newpage
\section{Przegląd dotychczasowych rozwiązań}
Problem sterowania robotami z wykorzystaniem informacji zwrotnej z otoczenia sięga początków współczesnej robotyki. Chcąc zapewnić możliwość pracy robota w środowiskach, w których nie mamy wystarczającej wiedzy na temat położenia obiektów z jakimi ma wchodzić w interakcje, konieczne jest zaimplementowanie podstawowej percepcji. Najprostszy przypadek może stanowić robot wyposażony w czujnik koloru w zadaniu sortowania kolorowych kulek. Wiele rozwiązań jednak opiera się na wykorzystaniu danych i przetworzeniu obrazu z kamery i czujników w taki sposób, aby robot był w stanie rozpoznać lub zlokalizować wskazane obiekty. Taką analizę obrazu nazywany widzeniem maszynowym lub CV od angielskich słów \emph{Computer Vision}.

\subsection{Widzenie maszynowe}
W dziedzinie widzenia maszynowego wyróżniamy kilka typów problemów związanych z obrazami, takich jak: detekcja, lokalizacja i segmentacja.

\begin{itemize}
\item \textbf{Detekcja} dostarcza nam informację czy wskazany obiekt znajduje się na obrazie,
a także w niektórych przypadkach, otrzymujemy prawdopodobieństwo jego wystąpienia. Nie zapewnia nam jednak informacji, gdzie dokładnie ten obiekt się znajduje.

\item \textbf{Lokalizacja} często pojawia się w parze z detekcją. Wynika to z faktu, że gdy mamy rozpoznany dany obiekt, chcemy pozyskać także informację o jego położeniu, co otrzymujemy w postaci współrzędnych lokalizacji, a czasem także wielkości.

\item \textbf{Semantyczna segmentacja} niesie nam odpowiedź o położeniu oraz kształcie obiektów. Załóżmy, że mamy trzy różne obiekty w puli do rozpoznania i wszystkie znajdują się na jednym obrazie, z czego dwa z nich są na nim po jednej sztuce, a ostatni – w dwóch. Semantyczna segmentacja umożliwi nam znalezienie położenia ich wszystkich, natomiast nadal nie wiemy czy jakiś z nich nie wystąpił w więcej niż jednej sztuce, przez co niemożliwe jest określenie ilości.

\item \textbf{Segmentacja z uwzględnieniem instancji} powstała jako rozwiązanie problemu
opisanego powyżej. Niesie nam odpowiedź, która oprócz podania dokładnego
położenia i kształtu obiektów także uwzględnia ich ilość.
\end{itemize} 

Wymieniowe wyżej problemy należą do dość złożonych zadań wymagających sporej mocy obliczeniowej. Z pomocą przychodzą algorytmy takie jak SIFT (w celu detekcji i lokalizacji danych obiektów na podstawie podobieństwa cech) czy metody uczenia nienadzorowanego (pomocne przy segmentacji). Często jednak z pomocą przychodzą sztuczne sieci neuronowe, a szczególnie popularne są sieci zawierające operację splotu, czyli sieci konwolucyjne.

\subsection{Sztuczne sieci neuronowe}


\subsection{Uczenie przez wzmacnianie}
Uczenie przez wzmacnianie zwane też uczeniem z krytykiem lub \emph{Reinforcement Learning} opisuje metody uczenia maszynowego, w którym optymalne rozwiązanie danego zadania powstaje na skutek interakcji agenta ze środowiskiem. W zastosowaniach tego podejścia w robotyce, środowiskiem jest symulacja lub rzeczywiste otoczenie, z którym agent (robot) wchodzi w interakcję. Celem jest maksymalizacja zysku ściśle związanego z pożądanym rezultatem. W ten sposób robot próbując osiągnąć jak najwyższy wynik uczy się rozwiązywać zadany problem.

\begin{figure}[h]
\centering
\includegraphics[width=10cm]{rl}
\caption{Przedstawienie zasady działania metod \emph{Reinforcement Learning} w postaci grafu: agent podejmuje działanie w danym środowisku co jest interpretowane jako zysk i reprezentacja stanu, które trafiają z powrotem do agenta. W dalej idących implementacjach algorytmu występują także obserwacje, czyli dostarczane agentowi informacje na temat środowiska.}
\end{figure}

Istnieje wiele wariantów algorytmów wykorzystujących \emph{Reinforcement Learning}, które możemy podzielić ze względu na kryterium optymalizacji na:
\begin{itemize}
\item \textbf{Oparte na polityce} \\
W tym podejściu przyjmowana jest polityka, którą należy zoptymalizować. Politykę możemy definiować jako odpowiedzialną za zachowanie agenta funkcję, która na wejściu przyjmuje obserwację, a na wyjściu zwraca akcję.  

\item \textbf{Oparte na funkcji wartości stanu} \\
Funkcja wartości stanu $V_{\pi}(s)$ określa jak wysoki będzie zysk, gdy zaczniemy ze stanu $s$ i będziemy opierać się o politykę $\pi$. 

\[V_{\pi}(s) = \mathbb{E}[R | s_{0}=s] = \mathbb{E} \left[\sum_{t=0}^n \gamma^t r_{t} | s_{0}=s \right]\]

W tym kryterium zostaje wprowadzona wartość $R$ jako całkowity \textbf{zysk}, który jest zdefiniowany jako suma poszczególnych przyszłych zysków

\[R = r_{0} + \gamma r_{1} + \gamma^2 r_{2} + \ldots + \gamma^n r_{n} = \sum_{t=0}^n \gamma^t r_{t} \quad gdzie \quad \gamma \in \langle 0, 1)\]

Ponieważ $\gamma$ jest mniejsza niż 1, odległe przyszłe zdarzenia są mniej istotne od zdarzeń w bliskiej i niedalekiej przyszłości. Jest to istotne, ponieważ zależy nam na njaszybszym możliwym uzyskaniu zysku.
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics{rubik}
\caption{Robot wykorzystujący techniki uczenia ze wzmocnieniem podczas układania kostki Rubika. Eksperyment został opisany w publikacji \emph{Learning Dexterous In-Hand Manipulation} - praca zbiorowa OpenAI.}
\end{figure}

\subsubsection{Wady uczenia przez wzmacnianie}
Jak już zostało wspomniane we wprowadzeniu, do wad podejścia wykorzystującego uczenie przez wzmocnienie w robotyce zaliczamy czas potrzebny do przetrenowania algorytmu oraz niewydajne i często chaotyczne ruchy, które są jego rezultatem.

\newpage
\section{Estymacja położenia obiektu}
Algorytm Iterative Closest Point (ICP) został szerzej opisany w artykule \emph{A Method for Registration of 3-D Shapes} przez Paula Besla i Neila McKaya w 1992 roku. Wcześniej została opublikowana praca \emph{Least-Squares Fitting of Two 3-D Point Sets} K.S Aryn at el. 
\\
Aby przejść do zadania estymacji położenia obiektu najpierw należy określić czy dysponujemy jego kształtem. Ponieważ w wielu zastosowaniach industrialnych przedstawionego algorytmu cecha ta jest wiadoma z góry, dla potrzeby tej pracy możemy przyjąć, że dysponujemy takim modelem. Mamy więc podane dwa kształty: rzeczywisty model obiektu pobrany z kamery RGBD jako chmura punktów oraz wyidealizowany model, który posłuży jako szablon.
\\
Mamy więc dwa zbiory punktów, z których każdy opisany jest przez współrzędne x, y, z.

\begin{figure}[h]
\centering
\includegraphics[width=10cm]{bunny}
\caption{Model obiektu i utworzona sztucznie chmura punktów.}
\end{figure}

\subsection{Wyznaczenie najbliższego punktu}

\textbf{Przypadek 1: dwa punkty}
\\
Odległość $d(p_{1}, p_{2})$ między dwoma punktami $p_{1} = (x_{1}, y_{1}, z_{1})$
i $p_{2} = (x_{2}, y_{2}, z_{2})$ jest dana w metryce euklidesowej wzorem: 
\[d(p_{1}, p_{2}) = ||p_{1} - p_{2}|| = \sqrt{(x_{2}-x_{1})^2+(y_{2}-y_{1})^2+(z_{2}-z_{1})^2}\]
\\
\textbf{Przypadek 2: punkt i zbiór punktów}
\\
Załóżmy, że A jest zbiorem n punktów oznaczonych jako $a_{i}$. Odległość między punktem p a zbiorem punktów A jest równa:
\[d(p, A) = \min_{i \in {1,...,n}} d(p, a_{i})\]
Wówczas najbliższy punkt spełnia warunek $d(p, a_{i}) = d(p, A)$
\\
\\
\textbf{Przypadek 3: punkt i odcinek}
\\
Załóżmy, że l jest odcinkiem łączącym punkty $p_{1}$ i $p_{2}$ Odległość między punktem p a odcinkiem l wynosi:
\[d(p, l) = \min_{u+v=1} ||up_{1}+vp_{2}-p|| \]
gdzie $u \in [0, 1]$ i $v \in [0, 1]$ 
\\
\\
\textbf{Przypadek 4: punkt i figura płaska}
\\
Załóżmy, że t jest trójkątem opisanym przez trzy punkty leżące na jego wierzchołkach: $p_{1} = (x_{1}, y_{1}, z_{1})$, $p_{2} = (x_{2}, y_{2}, z_{2})$ oraz $p_{3} = (x_{3}, y_{3}, z_{3})$ Odległość między punktem p a trójkątem t jest dana:
\[d(p, t) = \min_{u+v+w=1} ||up_{1}+vp_{2}-p+wp_{3}-p|| \]
gdzie $u \in [0, 1]$ i $v \in [0, 1]$ 
\\
\subsection{Transformacje}
Wspomniane algorytmy wyznaczenia najbliższego punktu mogą być rozwinięte do przestrzeni \emph{n}-wymiarowych. Aby dokonać takiej generalizacji do przestrzeni trójwymiarowej konieczne jest zastosowanie macierzy obrotu - rotacji \textbf{R} i wektora przesunięcia liniowego - translacji \textbf{T}.
\[
R = \begin{bmatrix}
R_{xx} & R_{xy} & R_{xz} \\
R_{yx} & R_{yy} & R_{yz} \\
R_{zx} & R_{zy} & R_{zz}
\end{bmatrix}
\qquad
T = \begin{bmatrix}
T_{x} \\ T_{y} \\ T_{z}
\end{bmatrix}
\]
\\
\textbf{Macierz rotacji}
\\
W zależności od tego jakim sposobem chcemy wykonywać działania mamy do wyboru trzy równoważne opcje obrotu danego kształtu w przestrzeni. Możemy w tym celu posłużyć się macierzą opartą o kąty obrotu wokół wszystkich trzech osi w układzie kartezjańskim, macierzą opartą o oś obrotu i kąt o jaki dany obiekt ma zostać obrócony lub kwaternionem.

\subsubsection{Sposób 1. Kąty Eulera}
Odnosząc się do rysunku, w pierwszym przypadku po lewej stronie mamy dane trzy osie układu kartezjańskiego i wykonujemy składowe obroty wokół każdej z nich. 
\[
R_{z}(\gamma) = \begin{bmatrix}
\cos \gamma & -\sin \gamma & 0 \\
\sin \gamma & \cos \gamma & 0 \\
0 & 0 & 1
\end{bmatrix}
\]
\[
R_{y}(\beta) = \begin{bmatrix}
\cos \beta & 0 & \sin \beta \\
0 & 1 & 0 \\
-\sin \beta & 0 & \cos \beta
\end{bmatrix}
\]
\[
R_{x}(\alpha) = \begin{bmatrix}
1 & 0 & 0 \\
0 & \cos \alpha & -\sin \alpha \\
0 & \sin \alpha & \cos \alpha \\
\end{bmatrix}
\]
\\
Mamy więc trzy obroty, które składają się na wynikową rotację.
\\
\[R = R_{z}(\gamma)R_{y}(\beta)R_{x}(\alpha)\]
Macierz rotacji przyjmuje wtedy postać

\[
R = \begin{bmatrix}
\cos \gamma \cos \beta & 
\cos \gamma \sin \beta \sin \alpha - \sin \gamma \cos \alpha &
\cos \gamma \sin \beta \cos \alpha + \sin \gamma \sin \alpha \\
\sin \gamma \cos \beta &
\sin \gamma \sin \beta \sin \alpha + \cos \gamma \cos \alpha &
\sin \gamma \sin \beta \cos \alpha - \cos \gamma \sin \alpha \\
-\sin \beta & \cos \beta \sin \alpha & \cos \beta \cos \alpha
\end{bmatrix}
\]

\subsubsection{Sposób 2. Oś obrotu i kąt}
W drugim - dysponujemy osią pożądanej rotacji, więc wykonujemy już tylko jeden obrót. Oś obrotu możemy przedstawić w postaci wektora jedenostkowego $\textbf{u} = [u_{x}, u_{y}, u_{z}]$ takiego, że $u_{x}^2+u_{y}^2+u_{z}^2 = 1$ 
Dla ułatwienia oznaczmy $c = \cos \theta$ i $s = \sin \theta$
\[
R = \begin{bmatrix}
c+u_{x}^2(1-c) & u_{x}u_{y}(1-c)-u_{z}s & u_{x}u_{z}(1-c)+u_{y}s \\
u_{y}u_{x}(1-c)-u_{z}s & c+u_{y}^2(1-c) & u_{y}u_{z}(1-c)+u_{x}s \\
u_{z}u_{x}(1-c)-u_{z}s & u_{z}u_{y}(1-c)-u_{x}s & c+u_{z}^2(1-c)
\end{bmatrix}
\]
\subsubsection{Sposób 3. Kwaternion}
W trzecim przypadku posługujemy się kwaternionem, który możemy zapisać w postaci sumy algebraicznej jako
\[q = a \cdot \textbf{e} + b \cdot \textbf{i} + c \cdot \textbf{j} + d \cdot \textbf{k} \]

gdzie $a, b, c, d \in \mathbb{R}$ zaś $\textbf{e, i, j, k}$ to pewne jednostki urojone, między którymi zachodzi zależność $i^2 = j^2 = k^2 = -1$.

Wówczas transformację tą możemy zapisać równoważnie w postaci macierzy:
\[
R = \begin{bmatrix}
a^2+b^2-c^2-d^2 & 2(bc-ad) & 2(bd+ac) \\
2(bc+ad) & a^2+c^2-b^2-d^2 & 2(cd-ab) \\
2(bd-ac) & 2(cd*ab) & a^2+d^2-b^2-c^2
\end{bmatrix}
\]

Mamy dwa zbiory punktów: \emph{p} i \emph{p'} w postaci macierzy o wymiarach 3 x N.

Wówczas równanie opisujące przejście punktu z jednego zbioru do drugiego wygląda następująco
\[p' = R p + T + N \]

Chcemy dobrać takie R i T, aby zminimalizować wyrażenie
\[ d^2(p, p') = \sum_{i=1}^n || p'_{i} - (Rp_{i}+T)||^2 \]

Niech 
\[p = \frac{1}{n} \sum_{i=1}^n p_{i} \]
\[p' = \frac{1}{n} \sum_{i=1}^n p'_{i} \]

Wówczas \emph{q} i \emph{q'} będą zbiorami odległości poszczególnych punktów w zbiorach \emph{p} i \emph{p'} od ich centroidów
\[q_{i} = p_{i} - p \]
\[q'_{i} = p'_{i} - p' \]

Mamy
\[d^2(q, q') = \sum_{i=1}^n || q'_{i} - Rq_{i}||^2 \]

Następnym krokiem jest wyznaczenie macierzy 3 x 3 
\[H = \sum_{i=1}^n q_{i} q_{i}^T \]

I rozłożenie jej na wartości osobliwe
\[H = UDV^T \]

\[X = VU^T \]

Jeśli wyznacznik \emph{det X} = 1, X stanowi macierz obrotu. W przeciwnym wypadku, gdy mamy do czynienia z wyznacznikiem \emph{det X}= -1 algorytm nie zwraca rozwiązań. 

\newpage
\subsection{Rozkład macierzy na wartości osobliwe}

\subsection{Implementacja algorytmu}
\begin{verbatim}
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.preprocessing import MinMaxScaler
import random
from math import *
import trimesh
import os
import icp
import test

mesh = trimesh.load_mesh('bunny.stl')

density = 3000
dim = 3
translation = 0.5
rotation = 0.5

A = mesh.sample(density)
scaler = MinMaxScaler(feature_range=(0, 1))
A = scaler.fit_transform(A)

B = mesh.sample(density)
B = scaler.fit_transform(B)

t = np.random.rand(dim) * translation
B += t

R = test.rotation_matrix(np.random.rand(dim), 
			np.random.rand() * rotation)
B = np.dot(R, B.T).T

iterations = 15

for i in range(iterations):
  T, distances, iters = icp.icp(B, A, max_iterations=(i+1))

  C = np.ones((density, 4))
  C[:, 0:3] = B
  C = np.dot(T, C.T).T

  fig = plt.figure(figsize=(9, 9))
  ax = Axes3D(fig)
  ax.scatter(A[:, 0], A[:, 1], A[:, 2], c='b')
  ax.scatter(C[:, 0], C[:, 1], C[:, 2], c='r')

  fig.savefig('icp{}.png'.format(i))
\end{verbatim}

\includegraphics[width=\textwidth]{bunny_icp}
Działanie algorytmu ICP na przykładzie obiektu w kształcie królika.

\section{Bibliografia}
\begin{enumerate}
\item \emph{Legged Robots That Balance} - Marc H. Raibert
\item \emph{Learning Dexterous In-Hand Manipulation} - OpenAI
\item \emph{The Senses Considered as Perceptual Systems} - James J. Ginson
\item \emph{Biocybernetyka} - Ryszard Tadeusiewicz
\item \emph{A Method for Registration of 3-D Shapes} - Paul J. Besl, Neil D. McKay
\item \emph{Least-Squares Fitting of Two 3-D Point Sets} - K.S. Arun, T.S. Huang, S.D. Blostein
\item \emph{Linear Algebra with Applications} - Steven J. Leon
\end{enumerate}
\end{document}