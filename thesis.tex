\documentclass[12pt]{article}
\usepackage{polski}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{physics}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{xcolor}
\graphicspath{{./images/}}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{1,1,1}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\footnotesize\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\small,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}
\lstset{
  literate={ą}{{\k a}}1
  		   {Ą}{{\k A}}1
           {ż}{{\. z}}1
           {Ż}{{\. Z}}1
           {ź}{{\' z}}1
           {Ź}{{\' Z}}1
           {ć}{{\' c}}1
           {Ć}{{\' C}}1
           {ę}{{\k e}}1
           {Ę}{{\k E}}1
           {ó}{{\' o}}1
           {Ó}{{\' O}}1
           {ń}{{\' n}}1
           {Ń}{{\' N}}1
           {ś}{{\' s}}1
           {Ś}{{\' S}}1
           {ł}{{\l}}1
           {Ł}{{\L}}1
}

\begin{document}
\title{Rozpoznawanie i estymacja pozycji obiektu z użyciem systemu wizyjnego w zadaniu chwytania realizowanego przez manipulator}
\author{
	Praca inżynierska \\\\
	Adam Wołkowycki \\\\
	Promotor: Dr Adam Wolniakowski
}
\maketitle

\newpage
\section*{Abstract}
In the recent years, a rising trend in application of autonomous robots can be observed. Nowadays, many solutions intended to this part of market are developed and each of these is based on solutions that are hard to algorithmizable. The examples of this kind of tasks are image processing, object position estimation and configuration of kinematics. I am going to focus on them in my thesis.

The purpose of this thesis is to develop a solution resposible for recognition a given object and perform grasping it by a manipulator. The further development of the problem known as \emph{bin picking} aims at increasing a scope of industrial robots' operation in a context of their work.

Because the tasks I mentioned are often performed by a particular type of robots, in my work I am going to use UR5 manipulator by Danish manufacturer Universal Robots. There will be implemented algorithm on it that will be able to define a location of seen objects based on the data from the computer vision sensors. In my work I will be using code examples written in Python. Thanks to this, it will be possible to deeply verified in practice the working of a robot.

\newpage
\section*{Streszczenie}
W ostatnich latach można zaobserwować wzrastający trend na zastosowanie robotów o działaniu autonomicznym. Powstaje obecnie wiele rozwiązań przeznaczonych dla tego segmentu rynku, a każdy z nich opiera swoje działanie o rozwiązania, które są przyjęte jako trudno algorytmizowalne. Przykładami takich zadań są przetwarzanie obrazu, estymacja położenia obiektów i konfiguracja kinematyki robota. Właśnie na nich zamieram skupić się w mojej pracy.

Celem pracy jest opracowanie rozwiązania odpowiedzialnego za rozpoznawanie wskazanego obiektu i realizację chwycenia go przy wykorzystaniu manipulatora. Rozwinięcie problemu znanego pod nazwą \emph{bin picking} ma przede wszystkim na celu zwiększenie zakresu działania robotów przemysłowych w kontekście ich pracy.

Ponieważ zadania, o których wspomniałem są często realizowane przez określony rodzaj robotów, w swojej pracy zamierzam wykorzytać manipulator UR5 duńskiego producenta Universal Robots. Na nim zostanie zaimplementowany algorytm, który w oparciu o dane z systemu wizyjnego będzie w stanie określić położenie widzianych przedmiotów. W pracy posłużę się programami stworzonymi w języku Python. Dzięki temu działanie robota będzie można gruntownie zweryfikować w praktyce.

\newpage
\tableofcontents

\newpage
\section{Wprowadzenie}
Definiując robotykę jako dziedzinę wiedzy mamy na uwadze zajmowanie się urządzeniami mechanicznymi, które mogą być przystosowane do realizacji wielu różnych zadań przez zmianę programu nimi sterującego. Jest to podstawowa różnica pozwalająca wyodrębnić robotykę na tle sztywnej automatyzacji zajmującej się maszynami przeznaczonymi do wykonywania jednego rodzaju zadań przez dłuższy czas. Współczesna robotyka stanowi przede wszystkim zbiór pewnych rozwiązań ukierunkowanych na tworzenie i sterowanie robotami. Mając to na uwadze, możemy wyróżnić dwa problemy: natury konstrukcyjnej i sterowania. Ponieważ wielu producentów z branży robotyki oferuje szeroką gamę rozwiązań spełniających potrzeby niniejszej pracy, posłużymy się manipulatorem przegubowym - najbardziej powszechną odmianą robota przemysłowego, w którego konstrukcję nie będziemy ingerować. Dlatego, aby zrozumieć koncepcje zawarte w tej pracy nie jest wymagana dogłębna znajomość budowy robota. Natomiast, jeśli chodzi o sterowanie to możemy ponownie wyodrębnić dwie podrzędne kategorie: manipulację i lokomocję. 

\begin{itemize}
\item \textbf{Manipulację} definiujemy jako zdolność do wykonywania precyzyjnych ruchów przez efektor końcowy robota w celu osiągnięcia wyznaczonego celu, np. przeniesienia przedmiotu procesowanego - tzw. zadanie \emph{pick and place}. Roboty posiadające tą cechę nazywamy mianem manipulatorów.

\item \textbf{Lokomocja} z kolei, jak sama nazwa wskazuje, opisuje zdolność do przemieszczania się platformy mobilnej. Jest kluczowa m.in. dla pojazdów AVG \emph{(Automated Guided Vehicles)}, łazików czy robotów kroczących. Ponieważ w swojej pracy zamierzam skupić się na robotach stacjonarnych, nie będę wracał więcej do tego tematu.
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=10cm]{classification}
\caption{Podział robotyki na odrębne zagadnienia.}
\end{figure}

Musimy jednak mieć na uwadze, że jest to dość ogólna i luźna klasyfikacja. Pierwotnie, definicję wyodrębniającą robotykę i automatyzację zaproponował John J. Craig, natomiast podział na manipulację i lokomocję został został przytoczony w książce Marca H. Raiberta \emph{Legged Robots That Balance} w ostatnim rozdziale w pytaniu \emph{Do Locomotion and Manipulation Have a Common Ground?} Tam też problem został rozwinięty w kontekście badań nad lokomocją i manipulacją oraz tego jak te dwie dziedziny wpływają na siebie nawzajem.

\subsection{Manipulacja a percepcja}
Gdy mamy już wyjaśnione ogólne zagadnienia możemy skupić się na danym zadaniu manipulatora. Załóżmy, że zadaniem robota jest pobranie kilku kostek z podajnika w miejscu A i przeniesienie ich w odpowiednie miejsca na palecie B, tzw. paletyzacja. Aby wykonać to zadanie możemy podejść do niego na dwa sposoby: zapewniając maszynie odpowiednią percepcję lub nie. 

\begin{itemize}
\item \textbf{Przypadek 1. Brak percepcji} \\
Brak percepcji oznacza brak danych wejściowych. Operujemy jedynie na wielkościach, które muszą zostać założone z góry. W tym przykładzie będą to: pozycje bazy i kiści manipulatora, podajnika, palety oraz rozmiar kostki. Z tego powodu, robot pozbawiony percepcji zawsze musi otrzymać od nas dokładne informacje odnośnie trajektorii ruchu.

\item \textbf{Przypadek 2. Percepcja z użyciem systemu wizyjnego} \\
Przez system wizyjny możemy rozumieć dowolny rodzaj kamery, jak również projektory chmur punktów i urządzenia nakładające na siebie kolory z danymi o głębokości obrazu, zwane kamerami RGBD. W takim przypadku operujemy znaczną ilością danych, dzięki czemu znajomość wymienionych wyżej wartości nie musi być konieczna - możemy oszacować te wartości używając metod widzenia maszynowego \emph{(Computer Vision).} Zastosowanie systemu wizyjnego możemy ponownie podzielić na dwa typy: z kamerą zamontowaną nieruchomo względem bazy robota oraz na jego narzędziu. Różnica między nimi polega głównie na tym, że w pierwszym przypadku widziane obiekty procesowane, jeśli robot ich bezpośrednio nie dotyka, pozostają w bezruchu względem kamery, a przez to nie zmienia się ich obraz. Przypadek z kamerą zamontowaną na kiści jest w stanie zapewnić więcej użytecznych ujęć, ale przez to jest też trudniejszy do zaimplementowania.
\end{itemize}

\begin{figure}[h]
\includegraphics[width=\textwidth]{basic}
\caption{Przykład zadania paletyzacji jako manipulacji przy braku percepcji. Po lewej widoczny jest kod programu stworzony w języku MELFA-BASIC IV. Po prawej na górze - ujęcie wykonane podczas pracy robota Kawasaki, na dole - widok palety do ukończeniu zadania. Symulacja została wykonana w środowisku Cosimir.}
\end{figure}

\begin{figure}[h]
\includegraphics[width=\textwidth]{bullet}
\caption{Przykład zadania \emph{bin picking} wykonywanego przez robota KUKA w oparciu o dane z systemu wizyjnego. Widoczne po lewej obrazy zostały wygenerowane sztucznie jako hipotetyczne dane z kamery umieszczonej nieruchomo względem bazy robota. Od góry - ujęcie powstałe jako bezpośrednia projekcja, obraz niosący dane o głębii i obraz poddany segmentacji. Symulacja została stworzona w języku Python z wykorzystaniem biblioteki \emph{pybullet}.}
\end{figure}

\subsection{Cele manipulacji}
Chociaż pierwsze w pełni sprawne manipulatory istnieją w sektorze industrialnym od dekad, wykorzystanie ich w zadaniu manipulacji stanowi wyzwanie, podczas gdy często stosowane rozwiązania nie wykorzystują w pełni ich potencjału. Dlatego chcąc określić potrzebę implementacji omawianych tu rozwiązań warto jest przyjrzeć się celom przed, którymi staje współczesna robotyka. Przede wszystkim chcemy, aby dane rozwiązanie było możliwie jak najbardziej \textbf{uniwersalne}, to znaczy działało poprawnie w różnych środowiskach, było odporne na zaszumienia i efektywne. Aby te cechy mogły zostać spełnione należy pewne kwestie muszą zostać rozwiązane, a należą do nich:

\begin{itemize}
\item \textbf{Zdolność do operowania wieloma różnymi obiektami.} Uniwersalność może zostać osiągnięta przez adaptację, czyli trwający w czasie proces, którego celem jest przystosowanie danego podmiotu do obserwowanego środowiska. Algorytmy działające zgodnie z tą zasadą stają się dość popularne szczególnie w ostatnich latach, a techniki wiodące prym na tym polu określane są mianem \emph{Reinforcement Learning} (w języku polskim przyjęły się nazwy uczenia ze wspomaganiem i uczenia z krytykiem). Nie zawsze stanowią jednak wydajne rozwiązanie zadania, głownie ze względu na wspomniany wyżej czas potrzebny do przetrenowania algorytmu. Problematyczne mogą być również dość chaotyczne, a przez to nieoptymalne i trudne do przewidzenia trajektorie jakie algorytmy tego typu generują.

\item \textbf{Sterowanie we wszystkich stopniach swobody}. Manipulator przemysłowy wykorzystany w tej pracy charakteryzuje się sześcioma stopniami swobody (bez chwytaka), co sprawia, że dla niektórych pozycji zadanie kinematyki odwrotnej będzie miało więcej niż jedno rozwiązanie i wiele możliwych trajektorii, z których nie wszystkie są optymalne.

\item \textbf{Odporność na zaszumione lub zniekształcone dane wejściowe}. Ograniczona rozdzielczość kamer RGBD, zaszumienie spowodowane światłem zewnętrznym i różne możliwe rozrzucenie obiektów komplikuje działanie algorytmu i aby proces przebiegał płynnie - musi zostać obsłużone.
\end{itemize}

Do powyższej listy moglibyśmy także dopisać zdolność do przenoszenia zachowań z symulacji do rzeczywistego świata, ale ponieważ nie we wszytkich problemach stosuje się metody symulacji - nie dopisywałem jej.

\subsection{Afordancje}
Zdolność oddziaływania na jakiś obiekt lub środowisko została określona mianem \textbf{afordancji} przez psychologa Jamesa J. Gibsona w 1966 roku w książce \emph{The Senses Considered as Perceptual Systems} i opisana w artykule \emph{The  Theory  of  Affordance.  In:  Perceiving,  Acting  and  Knowing  Toward  an Ecological Psychology}. Definicja ta przyjęła szerokie znaczenie w dziedzinach tj. psychologia, kognitywistyka, sztuczna inteligencja czy robotyka. Możemy zdefiniować afordancje na przykładzie ludzi, ale człowiek ze względu na swoją złożoność często pozostaje niewdzięcznym modelem. Dlatego dla naszych rozważań posłużymy się morskim bezkręgowcem - krabem \emph{Limulus polyphemus}. Przykład ten został opisany m.in. w \emph{Biocybernetyce} Ryszarda Tadeusiewicza. Korzystnymi cechami tego stawonoga z naszego punktu widzenia są posiadanie ośrodkowego układu nerwowego oraz oczu, z których impulsy jesteśmy w stanie śledzić. Tak więc, jesteśmy w stanie w przybliżeniu zobrazować jego pole widzenia. 

Co wobec tego widzi krab? Przede wszystkim \textbf{obiekty} stanowiące najczęściej zagrożenie lub będące przedmiotem łowów niezależnie od tła. W warunkach laboratoryjnych możemy także osiągnąć pobudzenie neuronalne za pomocą punktowego oświetlenia lub gwałtownych zmian światła. Krab nie widzi natomiast równomiernych gradientów oświetlenia czy monotonnego pastelowego tła. Jest to biologicznie uzasadnione, ponieważ informacje te nie są dla niego w żaden sposób niezbędne do życia. Presja ewolucyjna wywierana na mózgi zwierząt kształtowała się z najbardziej podstawowych powodów: żeby umożliwić im lepszą zdolność poruszania się i odpowiednią zdolność postrzegania otoczenia. Te umiejętności znajdowały odzwierciedlenie w najbardziej fundamentalnych cechach życiowych, czyli poszukiwaniu pożywienia i schronienia oraz ucieczki przed zagrożeniem. Dlatego jeśli prześledzimy drogę ewolucji prowadzącą do prostych bezkręgowców tj. glisty czy małże, zauważymy, że ich ośrodkowy układ nerwowy odpowiada głównie za kontrolę ruchu przez pobudzanie właściwych mięśni. A ponieważ nawet u najbardziej prymitywnych zwierząt ruch wykształcał się w parze ze zdolnością postrzegania środowiska - receptory reagujące na sygnały zewnętrzne (substancje chemiczne lub światło) przesyłają impulsy elektryczne do nerwów odpowiedzialnych za poruszanie się. 

Przykład kraba, mimo że może wydawać się surowy, niesie ze sobą istotne wskazówki na temat tego co powinniśmy rozumieć przez afordancje i jaka jest ich rola. Nikt dokładnie nie wie jak wyglądała droga rozwoju układu nerwowego prowadząca do zorganizowania jego struktury w mózg dorosłego człowieka, w którym ponad połowa neuronów odpowiada za kontrolę motoryczną i zmysły. Aby mieć władzę nad ciałem, mózg tworzy pewne odwzorowanie, które "mapuje" poszczególne części ciała na powierzchni kory mózgowej w miejscu zwojów \emph{gyrus postcentralis} (pól Brodmanna 1, 2 i 3). Mając taką mapę możemy zauważyć, że łączna powierzchnia dłoni i twarzy zajmuje na niej znacznie więcej miejsca niż pozostała reszta ciała. Innymi słowy, czynności wykonywane twarzą i dłońmi, tj. mimika, mowa czy manipulacja, angażują znaczne ilości neuronów co stanowi pewną miarę ich skomplikowania. Ostatnie badania stosujące techniki neuroobrazowania sugerują, że umiejętność "obchodzenia się z narzędziami", czyli zdolność do projektowania, planowania i korzystania z narzędzi wyewoluowała w lewej półkuli mózgu. Dlatego też pacjenci, u których zdiagnozowano uszkodzenia tej części, mimo że potrafią rozpoznać dany przedmiot, nie potrafią sobie wyobrazić jak go użyć i nie radzą sobie w tej kwestii lepiej niż szympansy. 

Tak więc, aby mogło zostać zrealizowane chwytanie -  wykorzystywane są informacje zawarte w obrazie. Pozwala to tłumaczyć dlaczego niektóre gatunki, przykładem tu mogą być wczesne homidy, wykształciły bardziej rozwinięty wzrok konieczny do wytypowania elementów otoczenia. Podobnie w robotyce, robot musi nauczyć się jak chwytać i operować uchwyconymi obiektami, tak aby osiągnąć wyznaczony cel. Różne przedmioty, np. młotek mogą być uchwycone na wiele różnych sposobów natomiast liczba optymalnych chwytów jest ograniczona w kontekście danego zadania. 

\newpage
\section{Przegląd dotychczasowych rozwiązań}
Problem sterowania robotami z wykorzystaniem informacji zwrotnej z otoczenia sięga początków współczesnej robotyki. Chcąc zapewnić możliwość pracy robota w środowiskach, w których nie mamy wystarczającej wiedzy na temat położenia obiektów z jakimi ma wchodzić w interakcje, konieczne jest zaimplementowanie podstawowej percepcji. Najprostszy przypadek może stanowić robot wyposażony w czujnik koloru w zadaniu sortowania kolorowych kulek. Wiele rozwiązań jednak opiera się na wykorzystaniu danych i przetworzeniu obrazu z kamery i czujników w taki sposób, aby robot był w stanie rozpoznać lub zlokalizować wskazane obiekty. Taką analizę obrazu nazywany widzeniem maszynowym lub CV od angielskich słów \emph{Computer Vision}.

\subsection{Widzenie maszynowe}
W dziedzinie widzenia maszynowego wyróżniamy kilka typów problemów związanych z obrazami, takich jak: detekcja, lokalizacja i segmentacja.

\begin{itemize}
\item \textbf{Detekcja} dostarcza nam informację czy wskazany obiekt znajduje się na obrazie,
a także w niektórych przypadkach, otrzymujemy prawdopodobieństwo jego wystąpienia. Nie zapewnia nam jednak informacji, gdzie dokładnie ten obiekt się znajduje.

\item \textbf{Lokalizacja} często pojawia się w parze z detekcją. Wynika to z faktu, że gdy mamy rozpoznany dany obiekt, chcemy pozyskać także informację o jego położeniu, co otrzymujemy w postaci współrzędnych lokalizacji, a czasem także wielkości.

\item \textbf{Semantyczna segmentacja} niesie nam odpowiedź o położeniu oraz kształcie obiektów. Załóżmy, że mamy trzy różne obiekty w puli do rozpoznania i wszystkie znajdują się na jednym obrazie, z czego dwa z nich są na nim po jednej sztuce, a ostatni – w dwóch. Semantyczna segmentacja umożliwi nam znalezienie położenia ich wszystkich, natomiast nadal nie wiemy czy jakiś z nich nie wystąpił w więcej niż jednej sztuce, przez co niemożliwe jest określenie ilości.

\item \textbf{Segmentacja z uwzględnieniem instancji} powstała jako rozwiązanie problemu opisanego powyżej. Niesie nam odpowiedź, która oprócz podania dokładnego położenia i kształtu obiektów także uwzględnia ich ilość.
\end{itemize} 

Wymieniowe wyżej problemy należą do dość złożonych zadań wymagających sporej mocy obliczeniowej. Z pomocą przychodzą algorytmy takie jak SIFT (w celu detekcji i lokalizacji danych obiektów na podstawie podobieństwa cech) czy metody uczenia nienadzorowanego (pomocne przy segmentacji). Często jednak z pomocą przychodzą sztuczne sieci neuronowe, a szczególnie popularne są sieci zawierające operację splotu, czyli sieci splotowe.

\subsection{Sztuczne sieci neuronowe}
Mówiąc o sieciach neuronowych w kontekście przedmiotów zajmujących się przetwarzaniem sygnałów mamy na myśli struktury matematyczne, które w pewnym stopniu przypominają budowę i działanie ludzkiego mózgu. Są one jedną z najbardziej popularnych technik współczesnej sztucznej inteligencji. Stosujemy je przede wszystkim wtedy, gdy nie znamy reguł rozwiązania danego problemu, ale dysponujemy zbiorem przykładowych zadań (zbiorem uczącym), które zostały poprawnie rozwiązane. 

Sztuczne sieci neuronowe składają się z wzajemnie połączonych neuronów, które są uproszczonymi modelami biologicznych odpowiedników. Jest to spowodowane tym, że rzeczywiste neurony są tworami niezmiernie skomplikowanymi i wymagającymi pod względem obliczeniowym. Gdy przedstawimy taką sieć w postaci grafu - jego wierzchołki będą reprezentować neurony, a krawędzie - poszczególne wagi sieci. Jej trening polega na znalezieniu takich wartości wag, które zapewnią poprawne rozwiązanie problemu. Zazwyczaj odbywa się to za pomocą algorytmu wstecznej propagacji błędów (ang. \emph{backpropagation}). Trenując sieć, generalizuje ona wiedzę na podstawie zbioru uczącego, od którego zależy jej późniejsze działanie.

\subsubsection*{Sieci jednokierunkowe}
Sztandarowym przykładem jest tu wielowarstwowa sieć jednokierunkowa, zwana również perceptronem wielowarstwowym lub MLP (ang. \emph{Multi-Layer Perceptron}). Jak sama nazwa wskazuje jest to struktura, w której zostały wyodrębnione \textbf{warstwy}, z których pierwszą nazywamy wejściową, środkowe - ukrytymi, a ostatnią - wyjściową. Warstwy te reprezentują kolejne poziomy abstrakcji tworząc model hierarchiczny. Najniższe z nich wykrywają proste cechy sygnału wejściowego, a najwyższe, opierając się o informacje z poprzednich warstw - są w stanie znaleźć pewne coraz bardziej abstrakcyjne cechy. 

Posługując się przykładem, załóżmy, że chcemy przetrenować pięciowarstwową sieć pod kątem zdolności do rozpoznawania obrazu. Przechodząc przez warstwy ukryte w najbardziej powszechnym przypadku pierwsza z nich wykryje krawędzie na podstawie różnic kolorów między pikselami, druga - korzystając z informacji o krawędziach będzie w stanie znaleźć kontury i zaokrąglenia, trzecia - pewne relacje pomiędzy nimi. Na ich podstawie z większym prawdopodobieństwem będzie mogła stwierdzić do jakiej klasy należy widziany obraz. W ten sposób, najczęściej w postaci rozkładu prawdopodobieństwa, dostaniemy informację o rozpoznanym obiekcie.

\begin{figure}[h]
\includegraphics[width=\textwidth]{neural_net}
\caption{Idea działania sieci neuronowej w zadaniu rozpoznawania obiektu. Przedstawiona została sieć składająca się z pięciu warstw: wejściowej, trzech ukrytych i wyjściowej. Na wejście trafiają dane, np. obraz lub chmura punktów, którą chcemy rozpoznać. Następnie kolejne warstwy ukryte wydobywają z nich coraz bardziej abstrakcyjne cechy. Pod koniec, informacja o przynależności do danej klasy A, B lub C jest zwracana na wyjściu sieci. Obraz został zaczerpnięty z książki \emph{Deep Learning: Systemy uczące się}.}
\end{figure}

\subsubsection*{Sieci splotowe}
Rodzaj sieci, na który szczególnie warto jest zwrócić uwagę stanowią sieci splotowe zwane częściej sieciami konwolucyjnymi, w skrócie CNN (ang. \emph{Convolutional Neural Network}). Te sieci zawdzięczają swoją popularność przede wszystkim dzięki wykorzystaniu do problemów związanych z analizą obrazów, przy których ich możliwości przewyższają zwykłe sieci jednokierunkowe. Ich nazwa wzięła się od operacji splotu (konwolucji), która jest wykorzystywana do ekstrakcji cech przez splot obrazu z filtrem. Sieci splotowe są z powodzeniem stosowane w rozpoznawaniu, klasyfikacji i segmentacji obrazów. Ponieważ możliwe jest przeprowadzenie operacji splotu na danych o zarówno jednym, dwóch oraz trzech wymiarach, użycie sieci CNN da się poszerzyć do analizy danych takich jak dźwięk, obrazy RGBD czy chmury punktów, a to z kolei implikuje ich przydatność w robotyce.

\subsubsection*{GraspNet}
Daleko idącym przykładem wykorzystania sieci neuronowych w robotyce jest projekt \emph{GraspNet}. Celem przedsięwzięcia było przetrenowanie modelu sztucznej inteligencji na zbiorze obrazów pochodzących z kamer RGBD przedstawiających przedmioty z możliwymi chwytami wyznaczonymi analitycznie. Twórcom projektu udało się zgromadzić 97280 obrazów RGBD zawierających ponad miliard możliwych pozycji chwytu.

\begin{figure}[h]
\centering
\includegraphics[width=10cm]{graspnet}
\caption{Wizualizacja możliwych chwytów wykrytych przez sieć \emph{GraspNet}. Na niebiesko oznaczone jest położenie szczęk chwytaka.}
\end{figure}

Zbiór ten został ujawniony i opublikowany pod nazwą \emph{GraspNet-1Billion}. W rzeczywistości \emph{GraspNet} jest modelem składającym się z kilku mniejszych sieci, który oprócz nich wykorzystuje funkcje tj. grupowanie, wyśrodkowywanie i filtrowanie danych. Tak więc, dla danych wejściowych w postaci chmury punktów przyporządkowuje pewną ilość chwytów jakie mogą zostać zastosowane podczas próby uchwycenia obiektów w niej przedstawionych. Możliwe chwyty zostają wyznaczone w sposób, na który mogą zostać przeprowadzone z użyciem chwytaka dwupalczastego.

\subsubsection*{Zalety sieci neuronowych}
Oprócz zdolności do rozwiązywania problemów, które nie są efektywnie algorytmizowalne w oparciu o matematyczne modelowanie wiedzy, do zalet sieci neuronowych możemy zaliczyć ich wszechstronność. To znaczy, dana struktura sieci może zostać przetrenowana do wielu różnych niezwiązanych ze sobą zadań. Jest to w pewnym sensie cecha wspólna wszystkich technik uczenia maszynowego wyróżniająca je na tle innych algorytmów.

\subsubsection*{Wady sieci neuronowych}
Sieci neuronowe mają wiele wad - trenowane algorytmem \emph{backpropagation} dokonują coraz mniejszych zmian w kolejnych warstwach idących wgłąb sieci (problem zanikającego gradientu) oraz łatwo przeuczają się, ponieważ większa liczba zmieniających się podczas uczenia sieci parametrów sprzyja przetrenowaniu. Skutkiem tego jest mniejsza wydajność proces ekstrakcji cech występujących w zbiorze uczącym. Trenując sieci neuronowe, w szczególności duże modele, nie do końca wiemy w jaki sposób przetwarzają one informacje. Pociąga to za sobą wiele implikacji, a patrząc z technicznej perspektywy, często nie jesteśmy pewni czy zastosowany przez nas model sieci nie mógłby zostać uproszczony. Gdybyśmy dysponowali większą wiedzą na temat przetwarzania przez nie informacji, najprawdopodobniej możliwa była by redukcja czasu, danych i zasobów obliczeniowych potrzebnych do ich uczenia. Są to najbardziej istotne czynniki wpływające na jakość działania modelu.

\newpage
\subsection{Uczenie przez wzmacnianie}
Uczenie przez wzmacnianie zwane też uczeniem z krytykiem lub \emph{Reinforcement Learning} opisuje metody uczenia maszynowego, w którym optymalne rozwiązanie danego zadania powstaje na skutek interakcji agenta ze środowiskiem. W zastosowaniach tego podejścia w robotyce, środowiskiem jest symulacja lub rzeczywiste otoczenie, z którym agent (robot) wchodzi w interakcję. Celem jest maksymalizacja zysku ściśle związanego z pożądanym rezultatem. W ten sposób robot próbując osiągnąć jak najwyższy wynik uczy się rozwiązywać zadany problem.

\begin{figure}[h]
\centering
\includegraphics[width=10cm]{rl}
\caption{Przedstawienie zasady działania metod \emph{Reinforcement Learning} w postaci grafu: agent podejmuje działanie w danym środowisku co jest interpretowane jako zysk i reprezentacja stanu, które trafiają z powrotem do agenta. W dalej idących implementacjach algorytmu występują także obserwacje, czyli dostarczane agentowi informacje na temat środowiska.}
\end{figure}

Istnieje wiele wariantów algorytmów wykorzystujących \emph{Reinforcement Learning}, które możemy podzielić ze względu na kryterium optymalizacji na oparte na polityce lub funkcji zmiany stanu.

\subsubsection*{Oparte na polityce}
W tym podejściu przyjmowana jest polityka, którą należy zoptymalizować. Politykę możemy definiować jako odpowiedzialną za zachowanie agenta funkcję, która na wejściu przyjmuje obserwację, a na wyjściu zwraca akcję.  

\subsubsection*{Oparte na funkcji wartości stanu}
Funkcja wartości stanu $V_{\pi}(s)$ określa jak wysoki będzie zysk, gdy zaczniemy ze stanu $s$ i będziemy opierać się o politykę $\pi$. 

\[V_{\pi}(s) = \mathbb{E}[R | s_0=s] = \mathbb{E} \left[\sum_{t=0}^n \gamma^t r_t | s_0=s \right]\]

W tym kryterium zostaje wprowadzona wartość $R$ jako całkowity \textbf{zysk}, który jest zdefiniowany jako suma poszczególnych przyszłych zysków

\[R = r_{0} + \gamma r_1 + \gamma^2 r_2 + \ldots + \gamma^n r_n = \sum_{t=0}^n \gamma^t r_t \quad gdzie \quad \gamma \in \langle 0, 1)\]

Ponieważ $\gamma$ jest mniejsza niż 1, odległe przyszłe zdarzenia są mniej istotne od zdarzeń w bliskiej i niedalekiej przyszłości. Jest to istotne, ponieważ zależy nam na najszybszym możliwym uzyskaniu zysku.

\subsubsection*{Deep Reinforcement Learning}
Związek uczenia przez wzmacnianie z robotyką stale się zacieśnia, zwłaszcza popularne w ostatnich latach stały się techniki wykorzystujące głębokie uczenie przez wzmacnianie, czyli \emph{Deep Reinforcement Learning}. Jest to w pewnym sensie połączenie dwóch omawianych wyżej metod: sieci neuronowych i \emph{Reinforcement Learningu} stosowane w sytuacjach, gdy mamy do czynienia z bardziej złożonymi problemami. Posługując się konkretnym przykładem, załóżmy że chcemy, aby robot na podstawie obrazu chmury punktów pochodzącej z kamery RGBD znalazł zadany przedmiot i go uchwycił. Jest to problem, na którym skupia się ta praca, szerzej znany pod nazwą \emph{bin picking}. Wówczas, gdy mamy wielowymiarowe obserwacje w postaci chmury punktów - niemożliwe się staje rozwiązanie go za pomocą tradycyjnego \emph{Reinforcement Learningu}. 

\begin{figure}[h]
\centering
\includegraphics{rubik}
\caption{Robot wykorzystujący techniki uczenia ze wzmacnianiem podczas układania kostki Rubika. Eksperyment został opisany w publikacji \emph{Learning Dexterous In-Hand Manipulation} i miał na celu przetrenowanie sztucznej inteligencji pod kątem umiejętności motorycznych - praca zbiorowa OpenAI.}
\end{figure}

\newpage
\subsubsection*{Zalety uczenia przez wzmacnianie}
Do niewątpliwych zalet uczenia przez wzmacnianie należy zdolność do tworzenia zachowań \textbf{emergentnych}. Przez emergencję rozumiemy powstawanie zachowań, do których instrukcje nie zostały bezpośrednio udzielone, a znalezione metodą prób i błędów i następnie z powodzeniem stosowane. Drugą ważną zaletą jest jego uniwersalność - metody oparte na \emph{Reinforcement Learning} mogą zostać z lepszym lub gorszym skutkiem użyte w większości problemów związanych z motoryką w robotyce, tj. nauka chodzenia, chwytania itp.

\subsubsection*{Wady uczenia przez wzmacnianie}
Jak już zostało wspomniane we wprowadzeniu, do wad podejścia wykorzystującego uczenie przez wzmacnianie w robotyce zaliczamy czas i często zasoby obliczeniowe potrzebne do przetrenowania algorytmu oraz niewydajne i często chaotyczne ruchy, które są jego rezultatem.

\newpage
\section{Estymacja położenia obiektu}
Algorytm Iterative Closest Point (ICP) został szerzej opisany w artykule \emph{A Method for Registration of 3-D Shapes} przez Paula Besla i Neila McKaya w 1992 roku. Wcześniej została opublikowana praca \emph{Least-Squares Fitting of Two 3-D Point Sets} K.S Aryn at el. 
\\
Aby przejść do zadania estymacji położenia obiektu najpierw należy określić czy dysponujemy jego kształtem. Ponieważ w wielu zastosowaniach industrialnych przedstawionego algorytmu cecha ta jest wiadoma z góry, dla potrzeby tej pracy możemy przyjąć, że dysponujemy takim modelem. Mamy więc podane dwa kształty: rzeczywisty model obiektu pobrany z kamery RGBD jako chmura punktów oraz wyidealizowany model, który posłuży jako szablon.
\\
Mamy więc dwa zbiory punktów, z których każdy opisany jest przez współrzędne x, y, z.

\begin{figure}[h]
\centering
\includegraphics[width=10cm]{bunny}
\caption{Model obiektu i utworzona sztucznie chmura punktów.}
\end{figure}

\subsection{Wyznaczenie najbliższego punktu}

\textbf{Przypadek 1: dwa punkty}
\\
Odległość $d(p_{1}, p_{2})$ między dwoma punktami $p_{1} = (x_{1}, y_{1}, z_{1})$
i $p_{2} = (x_{2}, y_{2}, z_{2})$ jest dana w metryce euklidesowej wzorem: 
\[d(p_{1}, p_{2}) = ||p_{1} - p_{2}|| = \sqrt{(x_{2}-x_{1})^2+(y_{2}-y_{1})^2+(z_{2}-z_{1})^2}\]
\\
\textbf{Przypadek 2: punkt i zbiór punktów}
\\
Załóżmy, że A jest zbiorem n punktów oznaczonych jako $a_{i}$. Odległość między punktem p a zbiorem punktów A jest równa:
\[d(p, A) = \min_{i \in {1,...,n}} d(p, a_{i})\]
Wówczas najbliższy punkt spełnia warunek $d(p, a_{i}) = d(p, A)$
\\
\\
\textbf{Przypadek 3: punkt i odcinek}
\\
Załóżmy, że l jest odcinkiem łączącym punkty $p_{1}$ i $p_{2}$ Odległość między punktem p a odcinkiem l wynosi:
\[d(p, l) = \min_{u+v=1} ||up_{1}+vp_{2}-p|| \]
gdzie $u \in [0, 1]$ i $v \in [0, 1]$ 
\\
\\
\textbf{Przypadek 4: punkt i figura płaska}
\\
Załóżmy, że t jest trójkątem opisanym przez trzy punkty leżące na jego wierzchołkach: $p_{1} = (x_{1}, y_{1}, z_{1})$, $p_{2} = (x_{2}, y_{2}, z_{2})$ oraz $p_{3} = (x_{3}, y_{3}, z_{3})$ Odległość między punktem p a trójkątem t jest dana:
\[d(p, t) = \min_{u+v+w=1} ||up_{1}+vp_{2}-p+wp_{3}-p|| \]
gdzie $u \in [0, 1]$ i $v \in [0, 1]$ 
\\
\subsection{Transformacje}
Wspomniane algorytmy wyznaczenia najbliższego punktu mogą być rozwinięte do przestrzeni \emph{n}-wymiarowych. Aby dokonać takiej generalizacji do przestrzeni trójwymiarowej konieczne jest zastosowanie macierzy obrotu - rotacji \textbf{R} i wektora przesunięcia liniowego - translacji \textbf{T}.
\[
R = \begin{bmatrix}
R_{xx} & R_{xy} & R_{xz} \\
R_{yx} & R_{yy} & R_{yz} \\
R_{zx} & R_{zy} & R_{zz}
\end{bmatrix}
\qquad
T = \begin{bmatrix}
T_{x} \\ T_{y} \\ T_{z}
\end{bmatrix}
\]

\subsubsection{Macierz rotacji}
W zależności od tego jakim sposobem chcemy wykonywać działania mamy do wyboru trzy równoważne opcje obrotu danego kształtu w przestrzeni. Możemy w tym celu posłużyć się macierzą opartą o kąty obrotu wokół wszystkich trzech osi w układzie kartezjańskim, macierzą opartą o oś obrotu i kąt o jaki dany obiekt ma zostać obrócony lub kwaternionem.

\subsubsection{Kąty Eulera}
Odnosząc się do rysunku, w pierwszym przypadku po lewej stronie mamy dane trzy osie układu kartezjańskiego i wykonujemy składowe obroty wokół każdej z nich. 
\[
R_{z}(\gamma) = \begin{bmatrix}
\cos \gamma & -\sin \gamma & 0 \\
\sin \gamma & \cos \gamma & 0 \\
0 & 0 & 1
\end{bmatrix}
\]
\[
R_{y}(\beta) = \begin{bmatrix}
\cos \beta & 0 & \sin \beta \\
0 & 1 & 0 \\
-\sin \beta & 0 & \cos \beta
\end{bmatrix}
\]
\[
R_{x}(\alpha) = \begin{bmatrix}
1 & 0 & 0 \\
0 & \cos \alpha & -\sin \alpha \\
0 & \sin \alpha & \cos \alpha \\
\end{bmatrix}
\]
\\
Mamy więc trzy obroty, które składają się na wynikową rotację.
\\
\[R = R_{z}(\gamma)R_{y}(\beta)R_{x}(\alpha)\]
Macierz rotacji przyjmuje wtedy postać

\[
R = \begin{bmatrix}
\cos \gamma \cos \beta & 
\cos \gamma \sin \beta \sin \alpha - \sin \gamma \cos \alpha &
\cos \gamma \sin \beta \cos \alpha + \sin \gamma \sin \alpha \\
\sin \gamma \cos \beta &
\sin \gamma \sin \beta \sin \alpha + \cos \gamma \cos \alpha &
\sin \gamma \sin \beta \cos \alpha - \cos \gamma \sin \alpha \\
-\sin \beta & \cos \beta \sin \alpha & \cos \beta \cos \alpha
\end{bmatrix}
\]

\subsubsection{Oś obrotu i kąt}
W drugim - dysponujemy osią pożądanej rotacji, więc wykonujemy już tylko jeden obrót. Oś obrotu możemy przedstawić w postaci wektora jedenostkowego $\textbf{u} = [u_{x}, u_{y}, u_{z}]$ takiego, że $u_{x}^2+u_{y}^2+u_{z}^2 = 1$ 
Dla ułatwienia oznaczmy $c = \cos \theta$ i $s = \sin \theta$
\[
R = \begin{bmatrix}
c+u_{x}^2(1-c) & u_{x}u_{y}(1-c)-u_{z}s & u_{x}u_{z}(1-c)+u_{y}s \\
u_{y}u_{x}(1-c)-u_{z}s & c+u_{y}^2(1-c) & u_{y}u_{z}(1-c)+u_{x}s \\
u_{z}u_{x}(1-c)-u_{z}s & u_{z}u_{y}(1-c)-u_{x}s & c+u_{z}^2(1-c)
\end{bmatrix}
\]

\subsubsection{Kwaternion}
W trzecim przypadku posługujemy się kwaternionem, który możemy zapisać w postaci sumy algebraicznej jako
\[q = a \cdot \textbf{e} + b \cdot \textbf{i} + c \cdot \textbf{j} + d \cdot \textbf{k} \]

gdzie $a, b, c, d \in \mathbb{R}$ zaś $\textbf{e, i, j, k}$ to pewne jednostki urojone, między którymi zachodzi zależność $i^2 = j^2 = k^2 = -1$.

Wówczas transformację tą możemy zapisać równoważnie w postaci macierzy:
\[
R = \begin{bmatrix}
a^2+b^2-c^2-d^2 & 2(bc-ad) & 2(bd+ac) \\
2(bc+ad) & a^2+c^2-b^2-d^2 & 2(cd-ab) \\
2(bd-ac) & 2(cd*ab) & a^2+d^2-b^2-c^2
\end{bmatrix}
\]

\newpage
\subsection{Rozkład macierzy według wartości osobliwych}
Celem rozkładu według wartości osobliwych, w skrócie rozkładu SVD (ang. \emph{Singular Value Decomposition}) w proponowanym algorytmie jest znalezienie macierzy rotacji, która stanowi transformację między chmurą punktów a szablonem, do którego próbujemy ją dopasować. Umożliwi to późniejsze uchwycenie przedmiotu przez chwytak manipulatora, ponieważ szablon jest obiektem, który jest znany, a więc możemy zamodelować poprawny proces chwytania.

Opisany w ten sposób problem stanowi jedynie pewien szczególny przypadek zastosowania rozkładu SVD. Ogólnie rzecz biorąc, SVD jest metodą pozwalającą na znalezienie rzędu macierzy, co teoretycznie może zostać wyznaczone metodą eliminacji Gaussa. To podejście jest jednak niepraktyczne, gdy ma zostać zrealizowane za pomocą metod numerycznych. Błędy biorą się często z niedoszacowań z powodu zaokrąglania wartości. Na przykład gdy rząd macierzy A jest niedoszacowany a U stanowi obliczoną numerycznie postać schodkową, wówczas możliwe jest, że U będzie miało niepoprawną liczbę niezerowych wierszy.

Najprostszym sposobem na zrozumienie rozkładu według wartości osobliwych jest zapisanie macierzy rzeczywistej A w postaci iloczynu trzech czynników, z których środkowy to macierz z pewnymi wartościami rosnącymi wzdłuż przekątnej. Taki rozkład jest zawsze możliwy, co można zapisać jako
\[A = UDV^T \]

Zakładając, że A jest macierzą $m \times n$, wówczas U stanie się macierzą ortogonalną o wymiarach $m \times m$, V - macierzą ortogonalną o wymiarach $n \times n$, a D macierzą $m \times n$, której wartości nieleżące na przekątnej są równe 0, a wartości na przekątnej spełniają zależność
\[\sigma_1 \geq \sigma_2 \geq \ldots \geq \sigma_n \geq 0\]

\[
D = \begin{bmatrix}
\sigma_1 & & & \\
& \sigma_2 & & \\
& & \ddots & \\
& & & \sigma_n \\\\
\end{bmatrix}
\]
Wartości $\sigma_i$ określone przez rozkład są różne i są nazywane \textbf{wartościami osobliwymi} macierzy A. Liczba niezerowych wartości osobliwych jest równa rzędowi macierzy A, a ich wielkość stanowi miarę jak bardzo ta macierz różni się od macierzy niższego rzędu. Z kolei kolumny macierzy U stanowią \textbf{lewostronne wektory osobliwe} a kolumny V - \textbf{prawostronne wektory osobliwe}.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{five_svd}
\caption{Graficzne przedstawienie zasady działania SVD na przykładzie obrazu ręcznie pisanej cyfry pięć o wymiarach 28 $\times$ 28 pikseli. Kolory zbliżone do fioletu reprezentują niskie wartości, a jaskrawe zbliżone ku żółci - wysokie. Możemy zauważyć, że sposób w jakim na obrazie macierzy D układają się coraz ciemniejsze kolory jest zgodny z założonym wzorem $\sigma_i \geq \sigma_{i+1} \geq 0$.}
\end{figure}

Twierdzenie związane z SVD zakłada, że jeśli A jest macierzą o wymiarach $m \times n$ możliwy jest jej rozkład według wartości osobliwych. Aby to wykazać, posłużmy się macierzą $A^T A$. Jest to macierz symetryczna, toteż wszystkie jej wartości własne są liczbami rzeczywistymi i istnieje taka macierz ortogonalna V, która ją diagonalizuje. Co więcej, jej wartości własne muszą być nieujemne. Aby to zobaczyć, niech $\lambda$ będzie wartością własną $A^T A$ a $\vb{x}$ wektorem własnym związanym z $\lambda$. Wynika z tego, że
\[||A \vb{x}||^2 = \vb{x}^{T}A^T A\vb{x} = \lambda\vb{x}^T \vb{x} = \lambda||\vb{x}||^2 \]

Aby wyznaczyć $\lambda$ z powyższego równania wykonujemy dzielenie
\[\lambda = \frac{||A\vb{x}||^2}{||\vb{x}||^2} \geq 0 \]

Podniesienie do kwadratu dowodzi, że $\lambda$ jest dodatnia. Załóżmy, że kolumny macierzy V zostały uporządkowane tak, że odpowiednie wartości własne spełniają zależność
\[\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_n \geq 0 \]

Wartości własne macierzy A są dane
\[\sigma_i = \sqrt{\lambda_i} \quad i = 1, 2,..., n \]

Niech r oznacza rząd macierzy A. Należy zauważyć, że macierz $A^T A$ też będzie rzędu r. Ponieważ $A^T A$ jest symetryczna jej rząd będzie równy liczbie niezerowych wartości własnych.
\[\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_r \geq 0 \qquad \lambda_{r+1} = \lambda_{r+2} = \ldots = \lambda_n \geq 0 \]

Ta sama zależność zachodzi dla wartości osobliwych
\[\sigma_1 \geq \sigma_2 \geq \ldots \geq \sigma_r \geq 0 \qquad \sigma_{r+1} = \sigma_{r+2} = \ldots = \sigma_n \geq 0 \]

Dla naszych dalszych rozważań konieczne jest przedstawienie poszczególnych czynników U, D i V  w rozbiciu na mniejsze macierze, z których są złożone. Zaczynając od macierzy V mamy
\[V_1 = (\vb{v}_1, \vb{v}_2,... \vb{v}_r), \quad V_1 = (\vb{v}_{r+1}, \vb{v}_{r+2},... \vb{v}_n) \]

\[
D_1 = \begin{bmatrix}
\sigma_1 & & & \\
& \sigma_2 & & \\
& & \ddots & \\
& & & \sigma_r
\end{bmatrix}
\qquad
D = \begin{bmatrix}
D_1 & 0 \\
0 & 0 \\
\end{bmatrix}
\]
Wektory $V_2$ stanowią wektory własne macierzy $A^T A$ związane z $\lambda = 0$
\[A_T A \vb{x}_i = 0 \quad i = r+1, r+2,..., n \]
\[AV_2 = 0 \]

Ponieważ V jest macierzą ortogonalną możemy zapisać
\[I = VV^T \]
\[A = AI = AVV^T \]

Zostało nam jeszcze skonstruować macierz ortogonalną U o wymiarach $m \times m$ taką, że
\[A = UDV^T \]

Co po przeniesieniu V na lewą stronę możemy równoważnie zapisać jako
\[AV = UD \]

Porównując pierwsze r kolumn po obu stronach możemy zauważyć
\[A \vb{v}_i = \sigma_i \vb{u}_i \quad i = 1, 2,..., n \]

Zatem jeśli, ustalimy 
\[\vb{u}_i = \frac{1}{\sigma_i}A \vb{v}_i \quad i = 1, 2,..., n \]

\[U_1 = (\vb{u}_1, \vb{u}_2,..., \vb{u}_r) \]

To będzie wynikać z tego
\[AV_1 = U_1 D_1 \]

\[\vb{u}_i^T \vb{u}_j = \left(\frac{1}{\sigma_i}\vb{v}_i^T A^T \right) \left(\frac{1}{\sigma_j}A \vb{v}_j \right)
= \frac{1}{\sigma_i \sigma_j}\vb{v}_i^T \left( A^T A \vb{v}_j \right)
= \frac{\sigma_j}{\sigma_i} \vb{v}_i^T \vb{v}_j 
= \delta_ij \]

Jeśli $\vb{u}_1,..., \vb{u}_m$ tworzy bazę ortonormalną dla $\mathbb{R}^m$ to U jest macierzą ortogonalną. Wówczas ostatnim krokiem jest wykazanie, że A rzeczywiście jet równe $UDV^T$
\[UDV^T = \begin{bmatrix}
U_1 & U_2
\end{bmatrix}
\begin{bmatrix}
D_1 & 0 \\
0 & 0 \\
\end{bmatrix}
\begin{bmatrix}
V_1^T \\
V_2^T \\
\end{bmatrix}
= U_1 D_1 V_1^T = AV_1 V_1^T = A \]

\subsubsection*{Interpretacja geometryczna rozkładu SVD}
W interpretacji geometrycznej wartości osobliwe mogą być rozumiane jako długości półosi elipsy na płaszczyźnie, co pokazano na rysunku. Wówczas zakładając, że A stanowi przekształcenie liniowe przestrzeni - macierze U i V reprezentują rotacje i odbicie tej przestrzeni, a D odpowiada za skalowanie. Innymi słowy, rozkład SVD rozkłada dowolne przekształcenie liniowe na złożenie funkcji trzech przekształceń: obrotu lub odbicia (V), skalowania (D) i kolejnego obrotu lub odbicia (U). W szczególności, jeśli wyznacznik poddawanej przekształcaniu macierzy jest dodatni, wówczas macierze U i V mogą odpowiadać zarówno za rotację jak i odbicie. Jeśli wyznacznik jest ujemny tylko jedna z nich odpowiada za odbicie, jeśli jest równy zero - odbicia nie ma wcale.

Taka interpretacja może również zostać uogólniona do $n$-wymiarowych przestrzeni euklidesowych. Wówczas wartości osobliwe dowolnej macierzy kwadratowej n $\times$ n staną się długościami półosi $n$-wymiarowej elipsoidy. Wartości osobliwe zawierają w sobie informację o długościach a wektory osobliwe - o kierunku półosi. Nie jest to jednak zbyt praktyczne rozwiązanie, dlatego mówiąc o rozkładzie SVD w proponowanym algorytmie będziemy posługiwać się uproszczonym SVD. Przede wszystkim, zakładamy, że dysponujemy szablonem, który ma dokładnie te same wymiary co szukany obiekt oraz szukany przedmiot jest bryłą sztywną (więc również jego szablon traktujemy w ten sam sposób). Te założenia sporo upraszczają. Przede wszystkim, odnosząc się do praktycznego zastosowania możemy pominąć operację skalowania.

\begin{figure}[h]
\centering
\includegraphics[width=10cm]{svd_geom}
\caption{Interpretacja geometryczna SVD macierzy A o wymiarach 2$\times$2. Celem jest przekształcenie przestrzeni pokazanej w lewym górnym rogu, tak aby uzyskać obraz widoczny w prawym górnym. Transformacja zostaje rozłożona przez SVD na trzy etapy: rotację, skalowanie i ponowną rotację.}
\end{figure}

\subsection{Algorytm Iterative Closest Point}
Mając teoretyczne podstawy za sobą możemy przejść do implementacji końcowego programu stworzonego w oparciu o algorytm ICP (ang. \emph{Iterative Closest Point}). Dla danej chmury punktów jego celem jest znalezienie położenia i orientacji obiektu, który przedstawia. W tej sytuacji dane wejściowe stanowią: chmura punktów i szablon szukanego obiektu, do którego chcemy dopasować widoczny przedmiot. Przez dopasowanie rozumiemy znalezienie transformacji (obrotu i przesunięcia) wykrytego obiektu względem początkowego położenia szablonu. Tak więc, mamy dwa zbiory punktów: \emph{p} i \emph{p'} w postaci macierzy o wymiarach 3 $\times$ n. Równanie opisujące przejście punktu z jednego zbioru do drugiego wygląda następująco
\[p' = R p + T + N \]

Gdzie R jest macierzą rotacji o wymiarach 3 $\times$ 3, T - wektorem przesunięcia o wymiarach 3 $\times$ 1 a N - wektorem szumu. Zakładamy też, że obrót odbywa się wokół początku układu. Chcemy dobrać takie R i T, aby zminimalizować wyrażenie
\[ d^2(p, p') = \sum_{i=1}^n || p'_{i} - (Rp_i+T)||^2 \]

Niech 
\[p = \frac{1}{n} \sum_{i=1}^n p_i \]
\[p' = \frac{1}{n} \sum_{i=1}^n p'_i \]

Wówczas \emph{q} i \emph{q'} będą zbiorami odległości poszczególnych punktów w zbiorach \emph{p} i \emph{p'} od ich centroidów
\[q_i = p_i - p \]
\[q'_{i} = p'_{i} - p' \]

Mamy
\[d^2(q, q') = \sum_{i=1}^n || q'_{i} - Rq_{i}||^2 \]

Następnym krokiem jest wyznaczenie macierzy 3 $\times$ 3 
\[H = \sum_{i=1}^n q_{i} q_{i}^{'T} \]

I rozłożenie jej na wartości osobliwe
\[H = UDV^T \]

Ponieważ, zgodnie z tym co zostało wspomniane podczas interpretacji geometrycznej rozkładu SVD, macierz D jest odpowiedzialna za skalowanie - podczas wyznaczenia wynikowej rotacji możemy ją odrzucić. W ten sposób otrzymujemy wyrażenie
\[R = VU^T \]

Jeśli wyznacznik \emph{det R} = 1, R stanowi macierz obrotu. W przeciwnym wypadku, gdy mamy do czynienia z wyznacznikiem \emph{det R}= -1 algorytm nie zwraca rozwiązań, ale ten przypadek z reguły nie występuje. 

Obliczona transformacja w postaci macierzy R i wektora $\vb{t}$ nie stanowi jeszcze ostatecznego rozwiązania algorytmu. Możemy jedynie stwierdzić, że poddany tej transformacji obiekt znajduje bliżej docelowego szablonu niż był do tej pory. Dążąc do ostatecznego wyniku, potrzebujemy ponownie wykonywać poprzednie kroki do momentu, aż odległości między korespondującymi punktami w zbiorach staną się akceptowalnie małe. Każdy taki etap nosi nazwę \textbf{iteracji}, dlatego też algorytm nazywany jest iteracyjnym.

\begin{figure}[h]
\centering
\includegraphics[width=10cm]{mean_dist}
\caption{Wykres średnich odległości korespondujących punktów między dwiema chmurami przeprowadzony dla rotacji w zakresie 0.25$\pi$. Zależność ta została powtórzona przez największą liczbę prób. Z przebiegu widać, że algorytm w największej liczbie przypadków znajdował ostateczną transformację przy osiemnastej iteracji.}
\end{figure}

\newpage
\subsection{Implementacja algorytmu}
Poniżej zamieszczona została minimalistyczna implementacja omówionej listy kroków w języku Python. Pełna wersja algorytmu ICP wraz z komentarzami znajduje się w sekcji \emph{Appendix}.

\lstinputlisting[language=Python]{transform_matrix.py}

Funkcja \emph{transform matrix} przyjmuje dwa argumenty w postaci macierzy A i B i. A jest macierzą obiektu, który chcemy dopasować do szablonu B. W rozważanym przypadku obie macierze są wymiaru 3 $\times$ n, czyli składają się z trzech kolumn, z których pierwsza zawiera wartości współrzędnej x, druga - y, a trzecia - z. Każdy wiersz w obu macierzach reprezentuje jeden z $n$ punktów. W linijkach piątej i szóstej następuje wyznaczenie centroidów A i B w postaci wektorów o długości trzy przez obliczenie średniej dla każdej kolumny współrzędnych. Macierze A1 i B1 zostają utworzone przez odjęcię od A i B ich centroidów. Pomaga nam to ustandaryzować wartości zmiennych. W linijce jedenastej liczony jest iloczyn nowo utworzonych macierzy (transponujemy A1, żeby miała tyle samo kolumn co B1 wierszy). Następnie następuje rozkład H według wartości osobliwych, który zwraca trzy czynniki w postaci macierzy U, D i Vt. Mnożąc przez siebie U i Vt (obie transponowane) otrzymujemy otrzymujemy macierz rotacji R. Warunek poniżej obsługuje specjalny przypadek odbicia. Wektor przesunięcia t jest liczony jako różnica centroidu B i obróconego przez R centroidu A. Funkcja zwraca R i t.

Gdy mamy zaimplementowane wyznaczanie transformacji dla pojedynczej iteracji możemy przejść do ostatecznego algorytmu ICP.

\lstinputlisting[language=Python]{icp.py}

Funkcja \emph{icp} przyjmuje cztery argumenty w tym jeden domyślny. Przyjmowane są zmienne A i B jako macierze o wymiarach 3 $\times$ n, maksymalna liczba iteracji oraz domyślnie ustawiony próg tolerancji. W linijkach 5-8 tworzone są macierze A1 i B1 jako czterokolumnowe odpowiedniki A i B. Dzieje się to przez skopiowanie wartości A i B i dostawienie kolumny jedynek. Linijki 11-23 zawierają główną pętlę programu wykonującą się maksymalnie \emph{max iterations} razy. Przed wejściem do pętli została zainicjalizowana zmienna \emph{prev error}, która będzie nadpisywana przy każdej iteracji. Następnie funkcja \emph{calculate distance} zwraca odległości euklidesowe między poszczególnymi punktami i ich indeksy. Będą one potem nam potrzebne do wyznaczenia średniego błędu dopasowania. Wyznaczana jest najlepsza transformacja T między A1 i B1, po czym zostaje ona zastosowana do przesunięcia A1. Pod koniec działania pętli liczony jest średni błąd jako średnia odległości między punktami. Jeśli wartość bezwzględna z różnicy średniego i poprzedniego błędu jest większa niż przyjęta toleracja, a dotychczasowa liczba iteracji mniejsza niż \emph{max iterations} - pętla wykonuje się ponownie. W momencie znalezienia wystarczająco dokładnej transformacji przed zakończeniem pętli - jest ona przerywana.

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{bunny_icp}
\caption{Działanie algorytmu ICP na przykładzie obiektu w kształcie królika.}
\end{figure}

\newpage
\section{Podsumowanie i wnioski}
W tej pracy została zaproponowana metoda estymacji położenia obiektu, którego modelem dysponujemy. Rozważony problem jest kluczowy na polu współczesnej robotyki, głównie przez trend do stosowania coraz bardziej "elastycznych" sposobów sterowania manipulatorami. Problem został rozbity na podstawowe zagadnienia, których rozwiązania również zostały przeprowadzone. Ponieważ jest nie jest to temat nowy, podczas pisania pracy było wykorzystanych wiele obecnie istniejących źródeł opisujących pełne lub cząstkowe rozwiązanie problemu. Tak czy inaczej, główną motywacją było znalezienie możliwie prostego i skutecznego rozwiązania, co zostało osiągnięte.

\subsection{Dalszy rozwój}
Mimo, że zaproponowana metoda estymacji położenia obiektu wykonała założone zadanie z powodzeniem, zastosowana technika posiada również pewne ograniczenia. Do najważniejszych z nich należy brak możliwości wyłonienia pożądanego przedmiotu ze zbioru zawierającego więcej niż jeden element. Tą kwestię można oczywiście rozwiązać stosując metody segmentacji. Mielibyśmy wtedy chmurę punktów rozbitą na kilka elementów, ale rodziłoby to kolejny problem - algorytm musiałby rozpoznać, który z nich stanowi żądany obiekt. W tym miejscu mogłaby zostać zaimplementowana logika odpowiadająca za obsługę przypadków w zależności czy w danych, które są sprawdzane mamy do czynienia z jednym, kilkoma lub żadnym szukanym obiektem. Podsumowując, dalszy rozwój tej pracy mógłby obejmować:

\begin{itemize}
\item Odszumienie i \textbf{segmentację} wejściowej chmury punktów w celu sprawdzenia z iloma elementami mamy do czynienia i gdzie się one znajdują.

\item \textbf{Rozpoznanie} jak bardzo dany przedmiot odbiega od swojego szablonu. To pozwoliłoby stwierdzić czy mamy do czynienia z właściwym obiektem. Jeśli tak nie jest - dalsza estymacja położenia nie ma sensu.
\end{itemize}

Wracając do tematu segmentacji, zadanie to dla niewielkiej liczby elementów może zostać zrealizowane przy pomocy technik analizy skupień, do którego zaliczamy algorytmy uczenia nienadzorowanego takie jak DBSCAN, grupowanie hierarchiczne i metodę $k$-średnich. Jeśli podział zostałby zrealizowany dla każdej liczby klastrów od 1 do $n$ (przy czym $n$ to założona maksymalna liczba klastrów) - wówczas do dyspozycji mielibyśmy miarę dopasowania do każdej z tych ewentualności. Taką miarą jest najczęściej kryterium sumy kwadratów w obrębie klastra WCSS \emph{(Within-Cluster Sum of Squared)}. Z niej możliwe stałoby się odczytanie optymalnej liczby elementów. 

Niestety, taka metoda jest zachłanna obliczeniowo, przez co zużywa sporo czasu i może być stosowana tylko do małych $n$. Wówczas, liczba klastrów rozważanych w każdej iteracji rośnie o jeden zgodnie z ciągiem arytmetycznym: 1, 2, 3, ... $n$. Ponieważ, w każdej iteracji jest mowa o innych klastrach ich sumaryczną ilość przedstawia ciąg: 1, 3, 6, 10, 15, ... $\sum n$. Innym powodem, który utrudnia pracę w wieloma elementami jest przede wszystkim to, że im większy jest ten zbiór - tym więcej charakterystycznych cech obiektów może być przysłonięte przez inne elementy. Trudno jest się z nim uporać w sposób obliczeniowy, ale może zostać w najprostszy sposób rozwiązany przez umieszczenie elementów na podajniku wibracyjnym.

\newpage
\section*{Bibliografia}
\begin{enumerate}
\item \emph{Wprowadzenie do robotyki} - John. J. Craig
\item \emph{Legged Robots That Balance} - Marc H. Raibert
\item \emph{The Senses Considered as Perceptual Systems} - James J. Ginson
\item \emph{Biocybernetyka} - Ryszard Tadeusiewicz
\item \emph{Odkrywanie właściwości sieci neuronowych} - Ryszard Tadeusiewicz
\item \emph{The Representation of Tool Use in Humans and Monkeys: Common and Uniquely Human Features}, "Journal of Neuroscience" - Scott H. Jonhson-Frey
\item \emph{Krótka historia rozumu. Od pierwszej myśli do rozumienia wszechświata} - Leonard Mlodinow
\item \emph{Deep Learning. Systemy uczące się} - Ian Goodfellow, Yoshua Bengio, Aaron Courville
\item \emph{Learning Dexterous In-Hand Manipulation} - OpenAI
\item \emph{Linear Algebra with Applications} - Steven J. Leon
\item \emph{A Singularly Valuable Decomposition} - Dan Kalman
\item \emph{A Method for Registration of 3-D Shapes} - Paul J. Besl, Neil D. McKay
\item \emph{Least-Squares Fitting of Two 3-D Point Sets} - K.S. Arun, T.S. Huang, S.D. Blostein
\end{enumerate}

\newpage
\section*{Appendix}
\subsection*{Kod programu}
\lstinputlisting[language=Python]{appendix.py}

\end{document}